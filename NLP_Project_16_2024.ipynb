{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description and information about the project goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/protago/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/protago/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/protago/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/protago/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import string\n",
    "import spacy\n",
    "from emotion import Emotion\n",
    "import json\n",
    "from wnaffect import WNAffect\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import heapq\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1\n",
    "Add task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all line numbers of lines in the specified topic. The topic_number argument must be given as a string. For example: '9'\n",
    "# Topic number 9 is politics\n",
    "def save_topic_lines(path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_lines = []\n",
    "    topic_number = str(topic_number)\n",
    "\n",
    "    with open(path_to_topic_file, 'r') as file:\n",
    "        \n",
    "        i = 1\n",
    "\n",
    "        for line in file:\n",
    "            if line[0] == topic_number:\n",
    "                topic_lines.append(i)\n",
    "            \n",
    "            i = i + 1\n",
    "\n",
    "    return topic_lines\n",
    "\n",
    "# Extracts all dialogue lines from a specific topic\n",
    "# if topic is 'all', every topic is extracted\n",
    "def extract_topic(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_lines = save_topic_lines(path_to_topic_file, topic_number)\n",
    "    topic_dialogue = []\n",
    "\n",
    "    with open(path_to_dialogue_file, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file):\n",
    "\n",
    "            if topic_number == 'all':\n",
    "                topic_dialogue.append(line)\n",
    "\n",
    "            elif topic_number != 'all':\n",
    "                if line_number in topic_lines:\n",
    "                    topic_dialogue.append(line)\n",
    "\n",
    "    return topic_dialogue\n",
    "\n",
    "# Creates a pandas dataframe for the dialogue data in a specific topic\n",
    "# Rows are dialogue lines. They are in the same order as in the original dialogues_text.txt file\n",
    "# Columns are utterances in that dialogue.\n",
    "def create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_dialogue = extract_topic(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "\n",
    "\n",
    "    #split_dialogue = [line.split('__eou__') for line in topic_dialogue]\n",
    "\n",
    "    split_dialogue = []\n",
    "\n",
    "    for line in topic_dialogue:\n",
    "\n",
    "        split_line = line.split('__eou__')\n",
    "\n",
    "        for i in range(len(split_line)):\n",
    "            \n",
    "            if split_line[i][0] == \" \":\n",
    "\n",
    "                new_line = split_line[i][1:]\n",
    "                split_line[i] = new_line\n",
    "\n",
    "            if split_line[i][-1] == \" \":\n",
    "\n",
    "                new_line = split_line[i][:-1]\n",
    "                split_line[i] = new_line\n",
    "\n",
    "\n",
    "        split_dialogue.append(split_line)\n",
    "\n",
    "\n",
    "    topic_dialogue_data = pd.DataFrame(split_dialogue)\n",
    "\n",
    "    return topic_dialogue_data\n",
    "\n",
    "# Saves the dataframe in excel format\n",
    "# This is just for not having to write the annoying file format\n",
    "def save_dataframe_as_excel(data, filename):\n",
    "\n",
    "    if '.xlsx' not in filename:\n",
    "        filename = filename + '.xlsx'\n",
    "\n",
    "    data.to_excel(filename, header=False, index=False)\n",
    "\n",
    "# Does everything above. Extracts the topic, makes it into a dataframe and saves in excel format\n",
    "# if topic number is 'all', every topic is extracted\n",
    "def extract_and_save_topic_dialogue(path_to_dialogue_file, path_to_topic_file, topic_number, filename):\n",
    "\n",
    "    topic_dialogue_data = create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "    save_dataframe_as_excel(topic_dialogue_data, filename)\n",
    "\n",
    "# Give topic number and name of file to save the data. The topic number can be given as a string or an integer.\n",
    "extract_and_save_topic_dialogue('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', 9, 'topic9data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for file \"topic9data.xlsx\":\n",
      "Size of vocabulary: 1567\n",
      "Number of utterances: 1611\n",
      "Average number of tokens per utterance: 6.848541278708876\n",
      "Average number of pronouns per utterance: 0.728739913097455\n",
      "Average number of agreement words per utterance: 0.032898820608317815\n",
      "Average number of negation words per utterance: 0.0446927374301676\n"
     ]
    }
   ],
   "source": [
    "# Open the data that was made with task1_save_topic.py\n",
    "def open_process_data(name_of_excel_file):\n",
    "    \n",
    "    data = pd.read_excel(name_of_excel_file)\n",
    "    rows, columns = data.shape\n",
    "    dialogues = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "\n",
    "        utterances = []\n",
    "\n",
    "        for column in range(columns):\n",
    "            if type(data.iat[row, column]) is str:\n",
    "                utterances.append(data.iat[row, column])\n",
    "            \n",
    "        dialogues.append(utterances)\n",
    "\n",
    "    return dialogues\n",
    "\n",
    "# Tokenizes, lowers and removes special characters from data\n",
    "def tokenize_data(dialogues):\n",
    "    \n",
    "    special = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '=', '+', '[', ']', '{', '}', ';', ':', '\"', \"'\", '<', '>', ',', '.', '/', '?', '\\\\', '|', '`', '~', '...']\n",
    "    tokenized_dialogues = []\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        tokenized_dialogue = []\n",
    "\n",
    "        for utterance in dialogue:\n",
    "            processed_tokenized_utterance = []\n",
    "            tokenized_utterance = word_tokenize(utterance)\n",
    "\n",
    "            for token in tokenized_utterance:\n",
    "                token.lower()\n",
    "                if (token not in special) and (len(token) != 1):\n",
    "                    processed_tokenized_utterance.append(token)\n",
    "                \n",
    "            tokenized_dialogue.append(processed_tokenized_utterance)\n",
    "        \n",
    "        tokenized_dialogues.append(tokenized_dialogue)\n",
    "    \n",
    "    return tokenized_dialogues\n",
    "\n",
    "# Calculates the vocabulary size for data. Data is given in the form that tokenize_data returns it\n",
    "def vocabulary_size(dialogues):\n",
    "    \n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    counted_words = []\n",
    "    vocabulary_size = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            for token in utterance:\n",
    "                if token not in counted_words:\n",
    "                    vocabulary_size += 1\n",
    "                    counted_words.append(token)\n",
    "                    #print(\"Unique token: \" + token)\n",
    "\n",
    "    return vocabulary_size\n",
    "\n",
    "# Calculates the number of utterances for a dialogue\n",
    "def count_utterances(dialogues):\n",
    "\n",
    "    num_of_utterances = 0\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        for utterance in dialogue:\n",
    "            num_of_utterances += 1\n",
    "\n",
    "    return num_of_utterances\n",
    "\n",
    "# Count average tokens per utterance for dialogue\n",
    "def count_avg_tokens_per_utterance(dialogues):\n",
    "\n",
    "    num_of_utterances = count_utterances(dialogues)\n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    total_tokens = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            total_tokens += len(utterance)\n",
    "\n",
    "    avg_tokens_per_utterance = total_tokens / num_of_utterances\n",
    "\n",
    "    return avg_tokens_per_utterance\n",
    "\n",
    "\n",
    "# Uses NLTK part of speech tagger to identify pronouns, counts\n",
    "# the number of pronouns and then the average per utterance\n",
    "def avg_pronouns_per_utterance(dialogues):\n",
    "    \n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    pronoun_count = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            tagged_utterance = pos_tag(utterance)\n",
    "\n",
    "            for (token, prp_tag) in tagged_utterance:\n",
    "                if prp_tag == ('PRP' or 'PRP$'):\n",
    "                    pronoun_count += 1\n",
    "\n",
    "    num_of_utterances = count_utterances(dialogues)\n",
    "    avg_prp = pronoun_count / num_of_utterances\n",
    "\n",
    "    return avg_prp\n",
    "\n",
    "\n",
    "# Didn't find any clear resource for agreement or negation wording.\n",
    "# There is nltk.metrics.agreement, but it is not for counting agreement words\n",
    "# There is also the option to try and find negation/agreement related words through wordnet, but it would also find words that are not specifially negation/agreement words\n",
    "# The custom list of agreement/negation words is subject to change\n",
    "# choice = 1 counts average number of agreement words\n",
    "# choice = 2 does the same for negation words\n",
    "def avg_agreement_negation_per_utterance(dialogues, choice):\n",
    "\n",
    "    agreement_words = ['yes', 'ok', 'sure', 'okay', 'agreed', 'agree']\n",
    "    negation_words = ['no', 'not', \"don't\", \"can't\", 'neither', ]\n",
    "\n",
    "    if choice == 1:\n",
    "        words_to_count = agreement_words\n",
    "    elif choice == 2:\n",
    "        words_to_count = negation_words\n",
    "    else:\n",
    "        print(\"Second argument: 1 for agreement words, 2 for negation words\")\n",
    "        return 0\n",
    "\n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    num_of_utterances = count_utterances(dialogues)\n",
    "    num_words_to_count = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            for token in utterance:\n",
    "                if token in words_to_count:\n",
    "                    num_words_to_count = num_words_to_count + 1\n",
    "    \n",
    "    avg_agreement_negation = num_words_to_count / num_of_utterances\n",
    "\n",
    "    return avg_agreement_negation\n",
    "\n",
    "# Prints all stats for a given topic\n",
    "def print_stats_from_excel(name_of_excel_file):\n",
    "\n",
    "    if '.xlsx' not in name_of_excel_file:\n",
    "        name_of_excel_file = name_of_excel_file + '.xlsx'\n",
    "\n",
    "    dialogues = open_process_data(name_of_excel_file)\n",
    "    vocab = vocabulary_size(dialogues)\n",
    "    utterances = count_utterances(dialogues)\n",
    "    tokens_per_utterance = count_avg_tokens_per_utterance(dialogues)\n",
    "    avg_prp = avg_pronouns_per_utterance(dialogues)\n",
    "    avg_agreement = avg_agreement_negation_per_utterance(dialogues, 1)\n",
    "    avg_negation = avg_agreement_negation_per_utterance(dialogues, 2)\n",
    "\n",
    "    print(\"Stats for file \\\"\" + name_of_excel_file + \"\\\":\")\n",
    "    print(\"Size of vocabulary: \" + str(vocab))\n",
    "    print(\"Number of utterances: \" + str(utterances))\n",
    "    print(\"Average number of tokens per utterance: \" + str(tokens_per_utterance))\n",
    "    print(\"Average number of pronouns per utterance: \" + str(avg_prp))\n",
    "    print(\"Average number of agreement words per utterance: \" + str(avg_agreement))\n",
    "    print(\"Average number of negation words per utterance: \" + str(avg_negation))\n",
    "\n",
    "\n",
    "# Give name of the excel data file, where the dialogue data saved with previous block is stored\n",
    "print_stats_from_excel('topic9data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2\n",
    "Add task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of person/organization named-entities per utterance: 0.011793916821849782\n"
     ]
    }
   ],
   "source": [
    "# Opens an excel data file saved in the format that task1_save_topic.py saves and\n",
    "# calculates the entity tags per utterance\n",
    "def avg_person_organization_entity_tags_per_utterance(excel_file_name):\n",
    "\n",
    "    if '.xlsx' not in excel_file_name:\n",
    "        excel_file_name = excel_file_name + '.xlsx'\n",
    "\n",
    "    entity_tagger = spacy.load(\"en_core_web_md\")\n",
    "    dialogues = open_process_data(excel_file_name)\n",
    "    num_entities = 0\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        for utterance in dialogue:\n",
    "            entity_tagged_utterance = entity_tagger(utterance)\n",
    "\n",
    "            for entity in entity_tagged_utterance.ents:\n",
    "                if entity.label_ == (\"ORG\" or \"PERSON\"):\n",
    "                    num_entities += 1\n",
    "\n",
    "    num_utterances = count_utterances(dialogues)\n",
    "    avg_ent_tag_per_utterance = num_entities / num_utterances\n",
    "\n",
    "    return avg_ent_tag_per_utterance\n",
    "\n",
    "# Give name of the excel data file, where the dialogue data is saved\n",
    "ent_tags_avg = avg_person_organization_entity_tags_per_utterance('topic9data')\n",
    "print(\"Average number of person/organization named-entities per utterance: \" + str(ent_tags_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3\n",
    "Add task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WNAffect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memos.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    101\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(emotions, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m wna \u001b[38;5;241m=\u001b[39m \u001b[43mWNAffect\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet-1.6/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwn-domains-3.2/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Comment here about the output and what this and validate(data) does\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memos.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WNAffect' is not defined"
     ]
    }
   ],
   "source": [
    "# Loads and parses dialogues from a text file dialogues_text.txt\n",
    "# separating each utterance by __eou__ and returns a list of lists (one per dialogue).\n",
    "def get_dialogs():\n",
    "\n",
    "    with open(\"dialogues_text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        dialogs = file.readlines()\n",
    "\n",
    "    parsed_dialogs = []\n",
    "    for dialog in dialogs:\n",
    "        d = dialog.split(\"__eou__\")\n",
    "        d = d[:-1]\n",
    "        parsed_dialogs.append(d)\n",
    "\n",
    "    return parsed_dialogs\n",
    "\n",
    "# Tokenizes each utterance and tags each word’s part of speech (POS).\n",
    "# Queries WNAffect to get emotions for each word in the utterance based on POS tags, accumulating any detected emotions in a list.\n",
    "# Returns list of emotions for utterance\n",
    "def get_emotions(utterance):\n",
    "\n",
    "    tokens = word_tokenize(utterance)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    emotions = []\n",
    "    for i in range(len(tokens)):\n",
    "        emo = wna.get_emotion(tokens[i], pos_tags[i][1])\n",
    "        if emo != None:\n",
    "            emotions.append(emo.name)\n",
    "            Emotion.printTree(Emotion.emotions[emo.name])\n",
    "            parent = emo.get_level(emo.level - 1)\n",
    "            print(\"parent: \" + parent.name)\n",
    "\n",
    "    return emotions\n",
    "\n",
    "# Compares the predicted emotions against expected labels from dialogues_emotion.txt.\n",
    "# Calculates accuracy and precision metrics by comparing the predicted emotions (from emos.json) with the labels.\n",
    "#   Consider as match if at least one emos.json label matches with tag from dialogues_emotion.txt.\n",
    "# validate uses a confusion matrix approach with:\n",
    "#   True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "# Prints accuracy and precision scores.\n",
    "def validate(emotions):\n",
    "\n",
    "    with open(\"dialogues_emotion.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        emotion_numbers = file.readlines()\n",
    "\n",
    "    en = []\n",
    "    for e in emotion_numbers:\n",
    "        a = e.split(\" \")\n",
    "        a = a[:-1]\n",
    "        en.append(a)\n",
    "\n",
    "    emo_tags = {0: \"no emotion\", 1: \"anger\", 2: \"disgust\", 3: \"fear\", 4: \"happiness\", 5: \"sadness\", 6: \"surprise\"}\n",
    "\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(emotions)):\n",
    "        for j in range(len(emotions[i])):\n",
    "            try:\n",
    "                tag = emo_tags[int(en[i][j])]\n",
    "                utterance_emo = emotions[i][j]\n",
    "                if tag in utterance_emo:\n",
    "                    tp += 1\n",
    "                elif tag == \"no emotion\":\n",
    "                    if len(utterance_emo) == 0:\n",
    "                        tn += 1\n",
    "                    else:\n",
    "                        fn += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            except IndexError:\n",
    "                print(\"index error: \" + str(i) + \" \" + str(j))\n",
    "\n",
    "    total = fp + fn + tp + tn\n",
    "\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = tp / (tp + fn)\n",
    "    \n",
    "    print(\"WNAffect scores: \")\n",
    "    print(\"Total utterance count: \" + str(total))\n",
    "    print(\"Accuracy: \" + str(round(accuracy ,3)))\n",
    "    print(\"Precision: \" + str(round(precision, 3)))\n",
    "\n",
    "    return\n",
    "\n",
    "# Calls get_dialogs to load dialogues.\n",
    "# Saves emotions fro each utterance to emos.json\n",
    "def save_emotions():\n",
    "    dialogs = get_dialogs()\n",
    "\n",
    "    emotions = []\n",
    "    for dialog in dialogs:\n",
    "        dialog_emo = []\n",
    "        for utterance in dialog:\n",
    "            emo = get_emotions(utterance)\n",
    "            dialog_emo.append(emo)\n",
    "        emotions.append(dialog_emo)\n",
    "\n",
    "    with open(\"emos.json\", \"w\") as f:\n",
    "        json.dump(emotions, f, indent=4)\n",
    "\n",
    "wna = WNAffect('wordnet-1.6/', 'wn-domains-3.2/')\n",
    "\n",
    "# Comment here about the output and what this and validate(data) does\n",
    "with open(\"emos.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "validate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4\n",
    "Add task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dialogues_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Comment here what this does\u001b[39;00m\n\u001b[1;32m      3\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m----> 4\u001b[0m dialogs \u001b[38;5;241m=\u001b[39m \u001b[43mget_dialogs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(analyzer.polarity_scores(\"Isn’t he the best instructor? I think he’s so hot. Wow! I really feel energized, dont’t you?\"))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dialog_sentiments \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mget_dialogs\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dialogs\u001b[39m():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdialogues_text.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         dialogs \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      8\u001b[0m     parsed_dialogs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/NLP/NLP_Project16_2024/project16env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dialogues_text.txt'"
     ]
    }
   ],
   "source": [
    "# Comment here what this does\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "dialogs = get_dialogs()\n",
    "\n",
    "#print(analyzer.polarity_scores(\"Isn’t he the best instructor? I think he’s so hot. Wow! I really feel energized, dont’t you?\"))\n",
    "\n",
    "dialog_sentiments = []\n",
    "for dialog in dialogs:\n",
    "    utterances = []\n",
    "    for utterance in dialog:\n",
    "        vs = analyzer.polarity_scores(utterance)\n",
    "        utterances.append(vs)\n",
    "        print(\"{:-<65} {}\".format(utterance, str(vs)))\n",
    "    dialog_sentiments.append(utterances)\n",
    "\n",
    "with open(\"sentiments.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(dialog_sentiments, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5\n",
    "Add task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dialogues_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Comment here\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[43mcompare_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m, in \u001b[0;36mcompare_and_save\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         dialogs\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     62\u001b[0m     compared_index\u001b[38;5;241m.\u001b[39mappend(dialogs)\n\u001b[0;32m---> 64\u001b[0m save_to_excel(compared_index, upper_level, sentiments, \u001b[43mget_dialogs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mget_dialogs\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dialogs\u001b[39m():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdialogues_text.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         dialogs \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      8\u001b[0m     parsed_dialogs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/NLP/NLP_Project16_2024/project16env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dialogues_text.txt'"
     ]
    }
   ],
   "source": [
    "# Add comments to all functions and what main does\n",
    "\n",
    "def save_upper_level_emotions():\n",
    "    dialogs = get_dialogs()\n",
    "\n",
    "    emotions = []\n",
    "    for dialog in dialogs:\n",
    "        dialog_emo = []\n",
    "        for utterance in dialog:\n",
    "            emo = get_emotions(utterance)\n",
    "            dialog_emo.append(emo)\n",
    "        emotions.append(dialog_emo)\n",
    "\n",
    "    with open(\"emos_upper_level.json\", \"w\") as f:\n",
    "        json.dump(emotions, f, indent=4)\n",
    "\n",
    "def get_emotions(utterance):\n",
    "\n",
    "    tokens = word_tokenize(utterance)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    emotion_tags = []\n",
    "    for i in range(len(tokens)):\n",
    "        emo = wna.get_emotion(tokens[i], pos_tags[i][1])\n",
    "        if emo != None:\n",
    "            emotion = get_upper_level_emotion(emo)\n",
    "            emotion_tags.append(emotion)\n",
    "            # Emotion.printTree(Emotion.emotions[emo.name])\n",
    "            # parent = emo.get_level(emo.level - 1)\n",
    "            # print(\"parent: \" + parent.name)\n",
    "\n",
    "    return emotion_tags\n",
    "\n",
    "def get_upper_level_emotion(emo):\n",
    "\n",
    "    parent = emo.get_level(emo.level - 1)\n",
    "\n",
    "    while parent.name != \"negative-emotion\" and parent.name != \"positive-emotion\" and parent.name != \"positive-emotion\" and parent.name != \"ambiguous-emotion\" and parent.name != \"neutral-emotion\":\n",
    "        parent = emo.get_level(parent.level - 1)\n",
    "    \n",
    "    if parent.name == \"ambiguous-emotion\" or parent.name == \"neutral-emotion\":\n",
    "        return 0\n",
    "    elif parent.name == \"negative-emotion\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def compare_and_save():\n",
    "\n",
    "    with open(\"emos_upper_level.json\", \"r\") as file:\n",
    "        upper_level = json.load(file)\n",
    "    \n",
    "    with open(\"sentiments.json\", \"r\") as file:\n",
    "        sentiments = json.load(file)\n",
    "\n",
    "    compared_index = []\n",
    "    for i in range(len(upper_level)):\n",
    "        dialogs = []\n",
    "        for j in range(len(upper_level[i])):\n",
    "            result = get_compared_result(sentiments[i][j][\"compound\"], upper_level[i][j])\n",
    "            dialogs.append(result)\n",
    "        compared_index.append(dialogs)\n",
    "\n",
    "    save_to_excel(compared_index, upper_level, sentiments, get_dialogs())\n",
    "    \n",
    "def save_to_excel(compared_index, emotion_values, sentiments, dialogs):\n",
    "    data = {}\n",
    "    compability_index_list = []\n",
    "    emotion_values_list = []\n",
    "    sentiments_list = []\n",
    "    utterances_list = []\n",
    "    for i in range(len(compared_index)):\n",
    "        compability_index_list.extend(compared_index[i])\n",
    "        for j in range(len(compared_index[i])):\n",
    "            if len(emotion_values[i][j]) == 0:\n",
    "                emotion_values_list.append(\"None\")\n",
    "            else:\n",
    "                emotion_values_list.append(emotion_values[i][j])\n",
    "            sentiments_list.append(sentiments[i][j][\"compound\"])\n",
    "            utterances_list.append(dialogs[i][j])\n",
    "    data[\"compability index\"] = compability_index_list\n",
    "    data[\"emotion value\"] = emotion_values_list\n",
    "    data[\"sentiment\"] = sentiments_list\n",
    "    data[\"utterance\"] = utterances_list\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(\"task5_data.xlsx\", index=False)\n",
    "\n",
    "def get_compared_result(sentiment_value, emotion_values):\n",
    "    if sentiment_value >= 0.05:\n",
    "        if 1 in emotion_values:\n",
    "            if 0 not in emotion_values and -1 not in emotion_values:\n",
    "                return 1\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif sentiment_value <= -0.05:\n",
    "        if -1 in emotion_values:\n",
    "            if 1 not in emotion_values and 0 not in emotion_values:\n",
    "                return 1\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if 0 in emotion_values:\n",
    "            if 1 not in emotion_values and -1 not in emotion_values:\n",
    "                return 1\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "# Comment here\n",
    "compare_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6\n",
    "Add task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to /home/protago/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy on the NLTK NPS corpus: 0.667\n"
     ]
    }
   ],
   "source": [
    "# Opens excel data and saves it into a multidimensional list\n",
    "# Use task1_save_topic.py to save topic dialogues in this excel form\n",
    "def open_process_data(name_of_data_file):\n",
    "    \n",
    "    data = pd.read_excel(name_of_data_file, header=None)\n",
    "    rows, columns = data.shape\n",
    "    dialogs = []\n",
    "\n",
    "    for row in range(rows):\n",
    "        utterances = []\n",
    "\n",
    "        for column in range(columns):\n",
    "            if (type(data.iat[row, column]) is str) & (data.iat[row, column] != \"\\n\"):\n",
    "                utterances.append(data.iat[row, column])\n",
    "            \n",
    "        dialogs.append(utterances)\n",
    "\n",
    "    return dialogs\n",
    "\n",
    "# Feature extraction function as specified in the NLTK organization book chapter 6, section 2.2\n",
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features\n",
    "\n",
    "# Trains the classifier as specified in the NLTK organization book chapter 6, section 2.2\n",
    "def train_NLTK_model():\n",
    "\n",
    "    nltk.download('nps_chat')\n",
    "    posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "\n",
    "    featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]\n",
    "    size = int(len(featuresets) * 0.1)\n",
    "    train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    print(\"Classifier accuracy on the NLTK NPS corpus: \" + str(nltk.classify.accuracy(classifier, test_set)))\n",
    "\n",
    "    return classifier\n",
    "\n",
    "# Classifies data from an excel file\n",
    "def classify_data(name_of_data_file):\n",
    "\n",
    "    if '.xlsx' not in name_of_data_file:\n",
    "        name_of_data_file = name_of_data_file + '.xlsx'\n",
    "\n",
    "    data = open_process_data(name_of_data_file)\n",
    "    classifier = train_NLTK_model()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for dialog in data:\n",
    "        dialog_predictions = []\n",
    "\n",
    "        for utterance in dialog:\n",
    "            utterance_features = dialogue_act_features(utterance)\n",
    "            prediction = classifier.classify(utterance_features)\n",
    "            dialog_predictions.append(prediction)\n",
    "\n",
    "        predictions.append(dialog_predictions)\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "# Saves predictions\n",
    "def save_to_json(predictions, saved_file_name):\n",
    "\n",
    "    with open(saved_file_name, \"w\") as file:\n",
    "        json.dump(predictions, file, indent=4)\n",
    "\n",
    "# Give name of the excel data file, where the dialogue data is saved\n",
    "predictions = classify_data('topic9data')\n",
    "save_to_json(predictions, 'topic9data_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7\n",
    "Add task description\n",
    "Pearson correlation and p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compound sentiment for Emotion is: -0.19733333333333333\n",
      "Compound sentiment for yAnswer is: 0.4125599510318254\n",
      "Compound sentiment for Continuer is: 0.14945422885572143\n",
      "Compound sentiment for whQuestion is: 0.10348610379266035\n",
      "Compound sentiment for System is: 0.06702743764172334\n",
      "Compound sentiment for Accept is: 0.36048351449275456\n",
      "Compound sentiment for Clarify is: 0.24550313324404716\n",
      "Compound sentiment for Emphasis is: 0.281300698757766\n",
      "Compound sentiment for nAnswer is: 0.0759587800369691\n",
      "Compound sentiment for Greet is: 0.019240740740740742\n",
      "Compound sentiment for Statement is: 0.19866943847343674\n",
      "Compound sentiment for Reject is: 0.12184455385363421\n",
      "Compound sentiment for Bye is: 0.17189492385786803\n",
      "Compound sentiment for Other is: 0.2135223988439305\n",
      "Compound sentiment for ynQuestion is: 0.1458641363683198\n",
      "[['yAnswer', 'stupefaction'], ['Continuer', 'stupefaction'], ['whQuestion', 'benevolence'], ['Accept', 'benevolence'], ['Clarify', 'stupefaction'], ['Emphasis', 'stupefaction'], ['nAnswer', 'stupefaction'], ['Greet', 'benevolence'], ['Statement', 'benevolence'], ['Reject', 'stupefaction'], ['Bye', 'benevolence'], ['Other', 'stupefaction'], ['ynQuestion', 'stupefaction']]\n"
     ]
    }
   ],
   "source": [
    "# Comment here\n",
    "def calculate_correlations(dialogue_acts_data_file, emotions_file, sentiments_file):\n",
    "\n",
    "    with open(dialogue_acts_data_file, 'r', encoding='utf-8') as file:\n",
    "        dialogue_acts_data = json.load(file)\n",
    "\n",
    "    with open(emotions_file, 'r', encoding='utf-8') as file:\n",
    "        emotions = json.load(file)\n",
    "\n",
    "    with open(sentiments_file, 'r', encoding='utf-8') as file:\n",
    "        sentiments = json.load(file)\n",
    "\n",
    "    # The first element in the list holds a dictionary that has emotions as keys and the number of times that\n",
    "    # emotion has appeared in the same utterance for that dialogue act as values\n",
    "    # The second element in the list holds the compound sentiment and third keeps track of how many compound\n",
    "    # sentiments were added together\n",
    "    correlations = {\n",
    "        \"Emotion\": [{}, 0, 0],\n",
    "        \"yAnswer\": [{}, 0, 0],\n",
    "        #\"yAnswer\" : [{}, 0, 0],\n",
    "        \"Continuer\": [{}, 0, 0],\n",
    "        \"whQuestion\": [{}, 0, 0],\n",
    "        \"System\": [{}, 0, 0],\n",
    "        \"Accept\": [{}, 0, 0],\n",
    "        \"Clarify\": [{}, 0, 0],\n",
    "        #\"Clarity\": [{}, 0, 0],\n",
    "        \"Emphasis\": [{}, 0, 0],\n",
    "        \"nAnswer\": [{}, 0, 0],\n",
    "        \"Greet\": [{}, 0, 0],\n",
    "        \"Statement\": [{}, 0, 0],\n",
    "        \"Reject\": [{}, 0, 0],\n",
    "        \"Bye\": [{}, 0, 0],\n",
    "        \"Other\" : [{}, 0, 0],\n",
    "        #\"Others\" : [{}, 0, 0],\n",
    "        \"ynQuestion\" : [{}, 0, 0],\n",
    "    }\n",
    "\n",
    "    # Iterate over each utterance in each dialogue\n",
    "    for dialogue in range(len(dialogue_acts_data)):\n",
    "        for utterance in range(len(dialogue_acts_data[dialogue])):\n",
    "\n",
    "            \n",
    "            dialogue_act = dialogue_acts_data[dialogue][utterance]\n",
    "            sentiment = sentiments[dialogue][utterance]\n",
    "\n",
    "            # If emotions list is empty, then emotions[dialogue][utterance][0] doens't exist,\n",
    "            # but if it has an emotion, then emotions[dialogue][utterance] is a list and not a string\n",
    "            if len(emotions[dialogue][utterance]) != 0:\n",
    "                emotion = emotions[dialogue][utterance][0]\n",
    "            else:\n",
    "                emotion = 'NaN'\n",
    "\n",
    "            # Adds emotion or increments emotion value in the correlations dict lists first dictionary\n",
    "            # If emotion is not found before for this dialogue act, it is added to the dict as a key, with a value of one.\n",
    "            # Otherwise the value for that key is incremented by one\n",
    "            # If emotion is nan, do nothing\n",
    "            if emotion == 'NaN':\n",
    "                a = 1\n",
    "            elif emotion in correlations[dialogue_act][0]:\n",
    "                correlations[dialogue_act][0][emotion] += 1\n",
    "            else:\n",
    "                correlations[dialogue_act][0][emotion] = 1\n",
    "\n",
    "            # Adds compound sentiment to the correlations dict lists second element\n",
    "            correlations[dialogue_act][1] += sentiment[\"compound\"]\n",
    "            correlations[dialogue_act][2] += 1\n",
    "\n",
    "    # This list stores lists, which have two elements. First is a dialogue act, second is the emotion that dialogue act has\n",
    "    # the highest correlation with, i.e. they appear the most together\n",
    "    highest_emotion_correlations = []\n",
    "\n",
    "    for dialogue_act in correlations:\n",
    "        \n",
    "        # Get average compound sentiment\n",
    "        correlations[dialogue_act][1] = correlations[dialogue_act][1] / correlations[dialogue_act][2]\n",
    "        print(\"Compound sentiment for \" + str(dialogue_act) + \" is: \" + str(correlations[dialogue_act][1]))\n",
    "\n",
    "        # For each dialogue act, find emotion that appears the most with it\n",
    "        if len(correlations[dialogue_act][0]) != 0:\n",
    "            highest_emotion_correlations.append([dialogue_act, max(correlations[dialogue_act][0], key=correlations[dialogue_act][0].get)])\n",
    "\n",
    "    return correlations, highest_emotion_correlations\n",
    "\n",
    "# Comment here\n",
    "correlations, highest_emotion_correlations = calculate_correlations('all_dialogues_predictions.json', 'emos.json', 'sentiments.json')\n",
    "print(highest_emotion_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8\n",
    "Add task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project16env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
