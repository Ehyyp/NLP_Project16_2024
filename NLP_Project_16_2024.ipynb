{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description and information about the project goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import string\n",
    "import spacy\n",
    "from emotion import Emotion\n",
    "import json\n",
    "from wnaffect import WNAffect\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import heapq\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('dict')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 save topic\n",
    "\n",
    "Processes the dialogue data and creates an excel file for a specified dailydialogue topic, or all topics. Rows are dialogues, columns utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all line numbers of lines in the specified topic. The topic_number argument must be given as a string. For example: '9'\n",
    "# Topic number 9 is politics\n",
    "def save_topic_lines(path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_lines = []\n",
    "    topic_number = str(topic_number)\n",
    "\n",
    "    with open(path_to_topic_file, 'r') as file:\n",
    "        \n",
    "        i = 1\n",
    "\n",
    "        for line in file:\n",
    "            if line[0] == topic_number:\n",
    "                topic_lines.append(i)\n",
    "            \n",
    "            i = i + 1\n",
    "\n",
    "    return topic_lines\n",
    "\n",
    "# Extracts all dialogue lines from a specific topic\n",
    "# if topic is 'all', every topic is extracted\n",
    "def extract_topic(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_lines = save_topic_lines(path_to_topic_file, topic_number)\n",
    "    topic_dialogue = []\n",
    "\n",
    "    with open(path_to_dialogue_file, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file):\n",
    "\n",
    "            if topic_number == 'all':\n",
    "                topic_dialogue.append(line)\n",
    "\n",
    "            elif topic_number != 'all':\n",
    "                if line_number in topic_lines:\n",
    "                    topic_dialogue.append(line)\n",
    "\n",
    "    return topic_dialogue\n",
    "\n",
    "# Creates a pandas dataframe for the dialogue data in a specific topic\n",
    "# Rows are dialogue lines. They are in the same order as in the original dialogues_text.txt file\n",
    "# Columns are utterances in that dialogue.\n",
    "def create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_dialogue = extract_topic(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "\n",
    "\n",
    "    #split_dialogue = [line.split('__eou__') for line in topic_dialogue]\n",
    "\n",
    "    split_dialogue = []\n",
    "\n",
    "    for line in topic_dialogue:\n",
    "\n",
    "        split_line = line.split('__eou__')\n",
    "\n",
    "        for i in range(len(split_line)):\n",
    "            \n",
    "            if split_line[i][0] == \" \":\n",
    "\n",
    "                new_line = split_line[i][1:]\n",
    "                split_line[i] = new_line\n",
    "\n",
    "            if split_line[i][-1] == \" \":\n",
    "\n",
    "                new_line = split_line[i][:-1]\n",
    "                split_line[i] = new_line\n",
    "\n",
    "\n",
    "        split_dialogue.append(split_line)\n",
    "\n",
    "\n",
    "    topic_dialogue_data = pd.DataFrame(split_dialogue)\n",
    "\n",
    "    return topic_dialogue_data\n",
    "\n",
    "# Saves the dataframe in excel format\n",
    "# This is just for not having to write the annoying file format\n",
    "def save_dataframe_as_excel(data, filename):\n",
    "\n",
    "    if '.xlsx' not in filename:\n",
    "        filename = filename + '.xlsx'\n",
    "\n",
    "    data.to_excel(filename, header=False, index=False)\n",
    "\n",
    "# Does everything above. Extracts the topic, makes it into a dataframe and saves in excel format\n",
    "# if topic number is 'all', every topic is extracted\n",
    "def extract_and_save_topic_dialogue(path_to_dialogue_file, path_to_topic_file, topic_number, filename):\n",
    "\n",
    "    topic_dialogue_data = create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "    save_dataframe_as_excel(topic_dialogue_data, filename)\n",
    "\n",
    "# Give topic number and name of file to save the data. The topic number can be given as a string or an integer.\n",
    "extract_and_save_topic_dialogue('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', 'all', 'all_dialogue_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 statistics\n",
    "\n",
    "Takes the excel format data from task 1 save topic as input and calculates the vocabulary size, total number of utterances and average number of tokens, pronouns and agreement/negation wording per utterances for those dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the data that was made with task1_save_topic.py\n",
    "def open_process_data(name_of_excel_file):\n",
    "    \n",
    "    data = pd.read_excel(name_of_excel_file)\n",
    "    rows, columns = data.shape\n",
    "    dialogues = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "\n",
    "        utterances = []\n",
    "\n",
    "        for column in range(columns):\n",
    "            if type(data.iat[row, column]) is str:\n",
    "                utterances.append(data.iat[row, column])\n",
    "            \n",
    "        dialogues.append(utterances)\n",
    "\n",
    "    return dialogues\n",
    "\n",
    "# Tokenizes, lowers and removes special characters from data\n",
    "def tokenize_data(dialogues):\n",
    "    \n",
    "    special = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '=', '+', '[', ']', '{', '}', ';', ':', '\"', \"'\", '<', '>', ',', '.', '/', '?', '\\\\', '|', '`', '~', '...']\n",
    "    tokenized_dialogues = []\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        tokenized_dialogue = []\n",
    "\n",
    "        for utterance in dialogue:\n",
    "            processed_tokenized_utterance = []\n",
    "            tokenized_utterance = word_tokenize(utterance)\n",
    "\n",
    "            for token in tokenized_utterance:\n",
    "                token.lower()\n",
    "                if (token not in special) and (len(token) != 1):\n",
    "                    processed_tokenized_utterance.append(token)\n",
    "                \n",
    "            tokenized_dialogue.append(processed_tokenized_utterance)\n",
    "        \n",
    "        tokenized_dialogues.append(tokenized_dialogue)\n",
    "    \n",
    "    return tokenized_dialogues\n",
    "\n",
    "# Calculates the vocabulary size for data. Data is given in the form that tokenize_data returns it\n",
    "def vocabulary_size(dialogues):\n",
    "    \n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    counted_words = []\n",
    "    vocabulary_size = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            for token in utterance:\n",
    "                if token not in counted_words:\n",
    "                    vocabulary_size += 1\n",
    "                    counted_words.append(token)\n",
    "                    #print(\"Unique token: \" + token)\n",
    "\n",
    "    return vocabulary_size\n",
    "\n",
    "# Calculates the number of utterances for a dialogue\n",
    "def count_utterances(dialogues):\n",
    "\n",
    "    num_of_utterances = 0\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        for utterance in dialogue:\n",
    "            num_of_utterances += 1\n",
    "\n",
    "    return num_of_utterances\n",
    "\n",
    "# Count average tokens per utterance for dialogue\n",
    "def count_avg_tokens_per_utterance(dialogues):\n",
    "\n",
    "    num_of_utterances = count_utterances(dialogues)\n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    total_tokens = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            total_tokens += len(utterance)\n",
    "\n",
    "    avg_tokens_per_utterance = total_tokens / num_of_utterances\n",
    "\n",
    "    return avg_tokens_per_utterance\n",
    "\n",
    "\n",
    "# Uses NLTK part of speech tagger to identify pronouns, counts\n",
    "# the number of pronouns and then the average per utterance\n",
    "def avg_pronouns_per_utterance(dialogues):\n",
    "    \n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    pronoun_count = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            tagged_utterance = pos_tag(utterance)\n",
    "\n",
    "            for (token, prp_tag) in tagged_utterance:\n",
    "                if prp_tag == ('PRP' or 'PRP$'):\n",
    "                    pronoun_count += 1\n",
    "\n",
    "    num_of_utterances = count_utterances(dialogues)\n",
    "    avg_prp = pronoun_count / num_of_utterances\n",
    "\n",
    "    return avg_prp\n",
    "\n",
    "\n",
    "# Didn't find any clear resource for agreement or negation wording.\n",
    "# There is nltk.metrics.agreement, but it is not for counting agreement words\n",
    "# There is also the option to try and find negation/agreement related words through wordnet, but it would also find words that are not specifially negation/agreement words\n",
    "# The custom list of agreement/negation words is subject to change\n",
    "# choice = 1 counts average number of agreement words\n",
    "# choice = 2 does the same for negation words\n",
    "def avg_agreement_negation_per_utterance(dialogues, choice):\n",
    "\n",
    "    agreement_words = ['yes', 'ok', 'sure', 'okay', 'agreed', 'agree']\n",
    "    negation_words = ['no', 'not', \"don't\", \"can't\", 'neither', ]\n",
    "\n",
    "    if choice == 1:\n",
    "        words_to_count = agreement_words\n",
    "    elif choice == 2:\n",
    "        words_to_count = negation_words\n",
    "    else:\n",
    "        print(\"Second argument: 1 for agreement words, 2 for negation words\")\n",
    "        return 0\n",
    "\n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    num_of_utterances = count_utterances(dialogues)\n",
    "    num_words_to_count = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            for token in utterance:\n",
    "                if token in words_to_count:\n",
    "                    num_words_to_count = num_words_to_count + 1\n",
    "    \n",
    "    avg_agreement_negation = num_words_to_count / num_of_utterances\n",
    "\n",
    "    return avg_agreement_negation\n",
    "\n",
    "# Prints all stats for a given topic\n",
    "def print_stats_from_excel(name_of_excel_file):\n",
    "\n",
    "    if '.xlsx' not in name_of_excel_file:\n",
    "        name_of_excel_file = name_of_excel_file + '.xlsx'\n",
    "\n",
    "    dialogues = open_process_data(name_of_excel_file)\n",
    "    vocab = vocabulary_size(dialogues)\n",
    "    utterances = count_utterances(dialogues)\n",
    "    tokens_per_utterance = count_avg_tokens_per_utterance(dialogues)\n",
    "    avg_prp = avg_pronouns_per_utterance(dialogues)\n",
    "    avg_agreement = avg_agreement_negation_per_utterance(dialogues, 1)\n",
    "    avg_negation = avg_agreement_negation_per_utterance(dialogues, 2)\n",
    "\n",
    "    print(\"Stats for file \\\"\" + name_of_excel_file + \"\\\":\")\n",
    "    print(\"Size of vocabulary: \" + str(vocab))\n",
    "    print(\"Number of utterances: \" + str(utterances))\n",
    "    print(\"Average number of tokens per utterance: \" + str(tokens_per_utterance))\n",
    "    print(\"Average number of pronouns per utterance: \" + str(avg_prp))\n",
    "    print(\"Average number of agreement words per utterance: \" + str(avg_agreement))\n",
    "    print(\"Average number of negation words per utterance: \" + str(avg_negation))\n",
    "\n",
    "\n",
    "# Give name of the excel data file, where the dialogue data saved with previous block is stored\n",
    "print_stats_from_excel('all_dialogue_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2\n",
    "\n",
    "Calculates the average number of person/organization named-entities per utterance, for dialogue data in the task 1 save topic excel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens an excel data file saved in the format that task1_save_topic.py saves and\n",
    "# calculates the entity tags per utterance\n",
    "def avg_person_organization_entity_tags_per_utterance(excel_file_name):\n",
    "\n",
    "    if '.xlsx' not in excel_file_name:\n",
    "        excel_file_name = excel_file_name + '.xlsx'\n",
    "\n",
    "    entity_tagger = spacy.load(\"en_core_web_md\")\n",
    "    dialogues = open_process_data(excel_file_name)\n",
    "    num_entities = 0\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        for utterance in dialogue:\n",
    "            entity_tagged_utterance = entity_tagger(utterance)\n",
    "\n",
    "            for entity in entity_tagged_utterance.ents:\n",
    "                if entity.label_ == (\"ORG\" or \"PERSON\"):\n",
    "                    num_entities += 1\n",
    "\n",
    "    num_utterances = count_utterances(dialogues)\n",
    "    avg_ent_tag_per_utterance = num_entities / num_utterances\n",
    "\n",
    "    return avg_ent_tag_per_utterance\n",
    "\n",
    "# Give name of the excel data file, where the dialogue data is saved\n",
    "ent_tags_avg = avg_person_organization_entity_tags_per_utterance('all_dialogue_data')\n",
    "print(\"Average number of person/organization named-entities per utterance: \" + str(ent_tags_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3 Emotion analysis using WNAffect\n",
    "\n",
    "Gives result for approaches A1 and A2 discussed in the report.\n",
    "\n",
    "In order to run this task, wordnet-1.6 and wn-domains-3.2 are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wna = WNAffect('wordnet-1.6/', 'wn-domains-3.2/')\n",
    "\n",
    "# Loads and parses dialogues from a text file dialogues_text.txt\n",
    "# separating each utterance by __eou__ and returns a list of lists (one per dialogue).\n",
    "def get_dialogs():\n",
    "\n",
    "    with open(\"ijcnlp_dailydialog/dialogues_text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        dialogs = file.readlines()\n",
    "\n",
    "    parsed_dialogs = []\n",
    "    for dialog in dialogs:\n",
    "        d = dialog.split(\"__eou__\")\n",
    "        d = d[:-1]\n",
    "        parsed_dialogs.append(d)\n",
    "\n",
    "    return parsed_dialogs\n",
    "\n",
    "def get_emotions(dialogs):\n",
    "    with open(\"dialogues_emotion.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        emotion_numbers = file.readlines()\n",
    "\n",
    "    en = []\n",
    "    for e in emotion_numbers:\n",
    "        a = e.split(\" \")\n",
    "        a = a[:-1]\n",
    "        en.append(a)\n",
    "\n",
    "    utterances = []\n",
    "    emotions = []\n",
    "    i = 0\n",
    "    for dialog in dialogs:\n",
    "        if len(dialog) != len(en[i]):\n",
    "            en[i].append(0)\n",
    "        utterances.extend(dialog)\n",
    "        emotions.extend(en[i])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return emotions\n",
    "\n",
    "# Compares the predicted emotions against expected labels from dialogues_emotion.txt.\n",
    "# Calculates accuracy and precision metrics by comparing the predicted emotions (from emos.json) with the labels.\n",
    "#   Consider as match if at least one emos.json label matches with tag from dialogues_emotion.txt.\n",
    "# validate uses a confusion matrix approach with:\n",
    "#   True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "# Prints accuracy and precision scores.\n",
    "def validate(emotions):\n",
    "\n",
    "    with open(\"dialogues_emotion.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        emotion_numbers = file.readlines()\n",
    "\n",
    "    en = []\n",
    "    for e in emotion_numbers:\n",
    "        a = e.split(\" \")\n",
    "        a = a[:-1]\n",
    "        en.append(a)\n",
    "\n",
    "    emo_tags = {0: \"no emotion\", 1: \"anger\", 2: \"disgust\", 3: \"fear\", 4: \"happiness\", 5: \"sadness\", 6: \"surprise\"}\n",
    "\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    no_emos = 0\n",
    "    no_predictions =  0\n",
    "\n",
    "    for i in range(len(emotions)):\n",
    "        for j in range(len(emotions[i])):\n",
    "            try:\n",
    "                tag = emo_tags[int(en[i][j])]\n",
    "                utterance_emo = emotions[i][j]\n",
    "                if len(utterance_emo) == 0:\n",
    "                    no_predictions += 1\n",
    "                if tag in utterance_emo:\n",
    "                    tp += 1\n",
    "                elif tag == \"no emotion\":\n",
    "                    no_emos += 1\n",
    "                    if len(utterance_emo) == 0:\n",
    "                        tn += 1\n",
    "                    else:\n",
    "                        fp += 1\n",
    "                elif len(utterance_emo) == 0:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            except IndexError:\n",
    "                print(\"index error: \" + str(i) + \" \" + str(j))\n",
    "\n",
    "    total = fp + fn + tp + tn\n",
    "\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    ratio_no_emos = no_emos / total\n",
    "    ratio_no_predictions = no_predictions / total\n",
    "    \n",
    "    print(\"WNAffect scores: \")\n",
    "    print(\"Total utterance count: \" + str(total))\n",
    "    print(\"Accuracy: \" + str(round(accuracy ,3)))\n",
    "    print(\"Precision: \" + str(round(precision, 3)))\n",
    "    print(\"No tag ratio: \" + str(round(ratio_no_emos, 3)))\n",
    "    print(\"No prediction ratio: \" + str(round(ratio_no_predictions, 3)))\n",
    "\n",
    "    return\n",
    "\n",
    "def validate_m(emotions):\n",
    "    \n",
    "    emotion_tags = get_emotions(get_dialogs())\n",
    "\n",
    "    emo_tags = {0: \"no emotion\", 1: \"anger\", 2: \"disgust\", 3: \"fear\", 4: \"happiness\", 5: \"sadness\", 6: \"surprise\"}\n",
    "\n",
    "    y_true = []\n",
    "    for tag in emotion_tags:\n",
    "        y_true.append(emo_tags[int(tag)])\n",
    "\n",
    "    y_pred = get_pred_class(emotions, y_true)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=np.nan)\n",
    "\n",
    "    print(\"Accuracy: \" + str(round(accuracy ,3)))\n",
    "    print(\"Precision: \" + str(round(precision, 3)))\n",
    "    print(\"Recall: \" + str(round(recall, 3)))\n",
    "\n",
    "def get_pred_class(emotions, y_true):\n",
    "\n",
    "    y_pred = []\n",
    "    for i in range(len(emotions)):\n",
    "        if y_true[i] in emotions[i]:\n",
    "            y_pred.append(y_true[i])\n",
    "        elif len(emotions[i]) == 0:\n",
    "            y_pred.append('no emotion')\n",
    "        elif 'negative-fear' in emotions[i] or 'ambiguous-fear' in emotions[i]:\n",
    "            if y_true[i] == 'fear':\n",
    "                y_pred.append('fear')\n",
    "            else:\n",
    "                y_pred.append(emotions[i][0])\n",
    "        else:\n",
    "            y_pred.append(emotions[i][0])\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Calls get_dialogs to load dialogues.\n",
    "# Saves emotions fro each utterance to emos.json\n",
    "def save_emotions():\n",
    "    dialogs = get_dialogs()\n",
    "\n",
    "    emotions = []\n",
    "\n",
    "    for dialog in dialogs:\n",
    "        dialog_emo = []\n",
    "        for utterance in dialog:\n",
    "            emo = get_emotions(utterance)\n",
    "            dialog_emo.append(emo)\n",
    "        emotions.append(dialog_emo)\n",
    "\n",
    "    with open(\"emos.json\", \"w\") as f:\n",
    "        json.dump(emotions, f, indent=4)\n",
    "\n",
    "def results_a1():\n",
    "    with open(\"emos.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    emos = []\n",
    "    for d in data:\n",
    "        for u in d:\n",
    "            emos.append(u)\n",
    "\n",
    "    validate_m(emos)\n",
    "\n",
    "def results_a2():\n",
    "    with open(\"emos_upperlevel.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    emos = []\n",
    "    for d in data:\n",
    "        for u in d:\n",
    "            emos.append(u)\n",
    "\n",
    "    validate_m(emos)\n",
    "\n",
    "results_a1()\n",
    "results_a2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4 Sentiment analysis with Vader sentiment analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves Vader sentiment analyser results to sentiments.json\n",
    "def save_sentiment():\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    dialogs = get_dialogs()\n",
    "\n",
    "    dialog_sentiments = []\n",
    "    for dialog in dialogs:\n",
    "        utterances = []\n",
    "        for utterance in dialog:\n",
    "            vs = analyzer.polarity_scores(utterance)\n",
    "            utterances.append(vs)\n",
    "            print(\"{:-<65} {}\".format(utterance, str(vs)))\n",
    "        dialog_sentiments.append(utterances)\n",
    "\n",
    "    with open(\"sentiments.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(dialog_sentiments, file, indent=4)\n",
    "\n",
    "# Reads sentiments.json and shows compound distribution of the results\n",
    "def results():\n",
    "\n",
    "    with open(\"sentiments.json\", 'r') as file:\n",
    "        sentiments = json.load(file)\n",
    "    \n",
    "    compounds = []\n",
    "    for dialog in sentiments:\n",
    "        for sentiment in dialog:\n",
    "            compounds.append(sentiment['compound'])\n",
    "\n",
    "    bins = np.linspace(-1,1)\n",
    "    data = compounds\n",
    "    plt.hist(data, bins=bins, edgecolor='black')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Vader sentiment analysis')\n",
    "    plt.xlabel('Compound score')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5 Correlation study between the sentiment and emotion states\n",
    "\n",
    "Prints values and shows both with none value results and without none values.\n",
    "\n",
    "In order to running this task, wordnet-1.6 and wn-domains-3.2 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wna = WNAffect('wordnet-1.6/', 'wn-domains-3.2/')\n",
    "\n",
    "# Saves level 4 emotions (ambiguous, neutral, negative and positive) to the emos_upper_level.json file\n",
    "def save_upper_level_emotions():\n",
    "    dialogs = get_dialogs()\n",
    "\n",
    "    emotions = []\n",
    "    for dialog in dialogs:\n",
    "        dialog_emo = []\n",
    "        for utterance in dialog:\n",
    "            emo = get_emotions(utterance)\n",
    "            dialog_emo.append(emo)\n",
    "        emotions.append(dialog_emo)\n",
    "\n",
    "    with open(\"emos_upper_level.json\", \"w\") as f:\n",
    "        json.dump(emotions, f, indent=4)\n",
    "\n",
    "# Tokenizes each utterance and tags each word’s part of speech (POS).\n",
    "# Gets level 4 emotions for all emotions found with WNAffect\n",
    "def get_emotions(utterance):\n",
    "\n",
    "    tokens = word_tokenize(utterance)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    emotion_tags = []\n",
    "    for i in range(len(tokens)):\n",
    "        emo = wna.get_emotion(tokens[i], pos_tags[i][1])\n",
    "        if emo != None:\n",
    "            emotion = get_upper_level_emotion(emo)\n",
    "            emotion_tags.append(emotion)\n",
    "            # Emotion.printTree(Emotion.emotions[emo.name])\n",
    "            # parent = emo.get_level(emo.level - 1)\n",
    "            # print(\"parent: \" + parent.name)\n",
    "\n",
    "    return emotion_tags\n",
    "\n",
    "# Helper function for searching the WNAffect emotion tree to level 4\n",
    "def get_upper_level_emotion(emo):\n",
    "\n",
    "    parent = emo.get_level(emo.level - 1)\n",
    "    if emo.name == 'love':\n",
    "        for child in emo.children:\n",
    "            print(child.name)\n",
    "\n",
    "    while parent.name != \"negative-emotion\" and parent.name != \"positive-emotion\" and parent.name != \"positive-emotion\" and parent.name != \"ambiguous-emotion\" and parent.name != \"neutral-emotion\":\n",
    "        parent = emo.get_level(parent.level - 1)\n",
    "    \n",
    "    if parent.name == \"ambiguous-emotion\" or parent.name == \"neutral-emotion\":\n",
    "        return 0\n",
    "    elif parent.name == \"negative-emotion\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Compares level 4 emotions to Vader sentiments.\n",
    "def compare_and_save():\n",
    "\n",
    "    with open(\"emos_upper_level.json\", \"r\") as file:\n",
    "        upper_level = json.load(file)\n",
    "    \n",
    "    with open(\"sentiments.json\", \"r\") as file:\n",
    "        sentiments = json.load(file)\n",
    "\n",
    "    compared_index = []\n",
    "    for i in range(len(upper_level)):\n",
    "        dialogs = []\n",
    "        for j in range(len(upper_level[i])):\n",
    "            result = get_compared_result(sentiments[i][j][\"compound\"], upper_level[i][j])\n",
    "            dialogs.append(result)\n",
    "        compared_index.append(dialogs)\n",
    "\n",
    "    save_to_excel(compared_index, upper_level, sentiments, get_dialogs())\n",
    "\n",
    "# Saves compatibility index, emotion value, sentiment, and corresponding utterance to the excel file task5_data.xlsx \n",
    "def save_to_excel(compared_index, emotion_values, sentiments, dialogs):\n",
    "    data = {}\n",
    "    compability_index_list = []\n",
    "    emotion_values_list = []\n",
    "    sentiments_list = []\n",
    "    utterances_list = []\n",
    "    for i in range(len(compared_index)):\n",
    "        compability_index_list.extend(compared_index[i])\n",
    "        for j in range(len(compared_index[i])):\n",
    "            if len(emotion_values[i][j]) == 0:\n",
    "                emotion_values_list.append(\"None\")\n",
    "            else:\n",
    "                emotion_values_list.append(emotion_values[i][j])\n",
    "            sentiments_list.append(sentiments[i][j][\"compound\"])\n",
    "            utterances_list.append(dialogs[i][j])\n",
    "    data[\"compability index\"] = compability_index_list\n",
    "    data[\"emotion value\"] = emotion_values_list\n",
    "    data[\"sentiment\"] = sentiments_list\n",
    "    data[\"utterance\"] = utterances_list\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(\"task5_data.xlsx\", index=False)\n",
    "\n",
    "# Helper function for crating conpatibility index\n",
    "def get_compared_result(sentiment_value, emotion_values):\n",
    "    if sentiment_value >= 0.05:\n",
    "        if 1 in emotion_values:\n",
    "            if 0 not in emotion_values and -1 not in emotion_values:\n",
    "                return 1\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif sentiment_value <= -0.05:\n",
    "        if -1 in emotion_values:\n",
    "            if 1 not in emotion_values and 0 not in emotion_values:\n",
    "                return 1\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if 0 in emotion_values:\n",
    "            if 1 not in emotion_values and -1 not in emotion_values:\n",
    "                return 1\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Results with none values\n",
    "def results():\n",
    "    df = pd.read_excel('task5_data.xlsx')\n",
    "    correlation = df['compability index']\n",
    "\n",
    "    counts = correlation.value_counts().sort_index()\n",
    "\n",
    "    print('incompatible: ', counts[0])\n",
    "    print('partial compability: ', counts[0.5])\n",
    "    print('Full compability: ', counts[1])\n",
    "\n",
    "    bin_labels = ['Incompatibility', 'Partial compatibility', 'Full compatibility']\n",
    "\n",
    "    plt.bar(bin_labels, counts, edgecolor='black')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Correlation distribution')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Results with no none values\n",
    "def results_none_removed():\n",
    "    df = pd.read_excel('task5_data.xlsx')\n",
    "    correlation = df['compability index']\n",
    "    emo_value = df['emotion value']\n",
    "\n",
    "    no_none_correlation = []\n",
    "    for i in range(len(correlation)):\n",
    "        if not pd.isna(emo_value[i]):\n",
    "            no_none_correlation.append(correlation[i])\n",
    "    \n",
    "    nncor = {}\n",
    "    nncor['compability index'] = no_none_correlation\n",
    "    df = pd.DataFrame(nncor)\n",
    "\n",
    "    counts = df['compability index'].value_counts().sort_index()\n",
    "\n",
    "    print('incompatible: ', counts[0])\n",
    "    print('partial compability: ', counts[0.5])\n",
    "    print('Full compability: ', counts[1])\n",
    "\n",
    "    bin_labels = ['Incompatibility', 'Partial compatibility', 'Full compatibility']\n",
    "\n",
    "    plt.bar(bin_labels, counts, edgecolor='black')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Correlation distribution')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "results()\n",
    "results_none_removed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6\n",
    "\n",
    "Uses NLTK naïve Bayes classifier trained with the NLTK nps chat data, to predict dialogue acts for each utterance in dialogue data that is in the task 1 save topic excel format. Saves all the predictions into json format, where the inner list elements are dialogues and outer list elements are predictions for the utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens excel data and saves it into a multidimensional list\n",
    "# Use task1_save_topic.py to save topic dialogues in this excel form\n",
    "def open_process_data(name_of_data_file):\n",
    "    \n",
    "    data = pd.read_excel(name_of_data_file, header=None)\n",
    "    rows, columns = data.shape\n",
    "    dialogs = []\n",
    "\n",
    "    for row in range(rows):\n",
    "        utterances = []\n",
    "\n",
    "        for column in range(columns):\n",
    "            if (type(data.iat[row, column]) is str) & (data.iat[row, column] != \"\\n\"):\n",
    "                utterances.append(data.iat[row, column])\n",
    "            \n",
    "        dialogs.append(utterances)\n",
    "\n",
    "    return dialogs\n",
    "\n",
    "# Feature extraction function as specified in the NLTK organization book chapter 6, section 2.2\n",
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features\n",
    "\n",
    "# Trains the classifier as specified in the NLTK organization book chapter 6, section 2.2\n",
    "def train_NLTK_model():\n",
    "\n",
    "    nltk.download('nps_chat')\n",
    "    posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "\n",
    "    featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]\n",
    "    size = int(len(featuresets) * 0.1)\n",
    "    train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    print(\"Classifier accuracy on the NLTK NPS corpus: \" + str(nltk.classify.accuracy(classifier, test_set)))\n",
    "\n",
    "    return classifier\n",
    "\n",
    "# Classifies data from an excel file\n",
    "def classify_data(name_of_data_file):\n",
    "\n",
    "    if '.xlsx' not in name_of_data_file:\n",
    "        name_of_data_file = name_of_data_file + '.xlsx'\n",
    "\n",
    "    data = open_process_data(name_of_data_file)\n",
    "    classifier = train_NLTK_model()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for dialog in data:\n",
    "        dialog_predictions = []\n",
    "\n",
    "        for utterance in dialog:\n",
    "            utterance_features = dialogue_act_features(utterance)\n",
    "            prediction = classifier.classify(utterance_features)\n",
    "            dialog_predictions.append(prediction)\n",
    "\n",
    "        predictions.append(dialog_predictions)\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "# Saves predictions\n",
    "def save_to_json(predictions, saved_file_name):\n",
    "\n",
    "    if '.json' not in saved_file_name:\n",
    "        saved_file_name = saved_file_name + '.json'    \n",
    "\n",
    "    with open(saved_file_name, \"w\") as file:\n",
    "        json.dump(predictions, file, indent=4)\n",
    "\n",
    "# Give name of the excel data file, where the dialogue data is saved\n",
    "predictions = classify_data('all_dialogue_data')\n",
    "save_to_json(predictions, 'all_dialogue_predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7\n",
    "\n",
    "This task does not meet the project details. I first started working on it in the wrong angle. Then I started calculating the pearson correlations, but ended up not calculating the correct correlations. The code creates 3 pandas dataframes, where each have dialogues as rows and utterances as columns. First dataframe has the utterances dialogue acts, another has the utterances compound sentiments and last one has the utterances emotions. Then the code removes empty columns and calculates the column wise pearson correlation.\n",
    "\n",
    "Requires emotion data from task 3, sentiment data from task 4 and dialogue act data from task 6. For all dialogue acts, counts the occurences of all different emotions and returns the emotion that appears the most with that dialogue act. Calculates the average compound sentiment for all dialogue acts by summing them together and dividing by the number of compound sentiments added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the emotion most associated with each dialogue act and the average compound sentiment of each dialogue act\n",
    "def calculate_matching(dialogue_acts_data_file, emotions_file, sentiments_file):\n",
    "\n",
    "    with open(dialogue_acts_data_file, 'r', encoding='utf-8') as file:\n",
    "        dialogue_acts_data = json.load(file)\n",
    "\n",
    "    with open(emotions_file, 'r', encoding='utf-8') as file:\n",
    "        emotions = json.load(file)\n",
    "\n",
    "    with open(sentiments_file, 'r', encoding='utf-8') as file:\n",
    "        sentiments = json.load(file)\n",
    "\n",
    "    # The first element in the list holds a dictionary that has emotions as keys and the number of times that\n",
    "    # emotion has appeared in the same utterance for that dialogue act as values\n",
    "    # The second element in the list holds the compound sentiment and third keeps track of how many compound\n",
    "    # sentiments were added together\n",
    "    correlations = {\n",
    "        \"Emotion\": [{}, 0, 0],\n",
    "        \"yAnswer\": [{}, 0, 0],\n",
    "        #\"yAnswer\" : [{}, 0, 0],\n",
    "        \"Continuer\": [{}, 0, 0],\n",
    "        \"whQuestion\": [{}, 0, 0],\n",
    "        \"System\": [{}, 0, 0],\n",
    "        \"Accept\": [{}, 0, 0],\n",
    "        \"Clarify\": [{}, 0, 0],\n",
    "        #\"Clarity\": [{}, 0, 0],\n",
    "        \"Emphasis\": [{}, 0, 0],\n",
    "        \"nAnswer\": [{}, 0, 0], \n",
    "        \"Greet\": [{}, 0, 0],\n",
    "        \"Statement\": [{}, 0, 0],\n",
    "        \"Reject\": [{}, 0, 0],\n",
    "        \"Bye\": [{}, 0, 0],\n",
    "        \"Other\" : [{}, 0, 0],\n",
    "        #\"Others\" : [{}, 0, 0],\n",
    "        \"ynQuestion\" : [{}, 0, 0],\n",
    "    }\n",
    "\n",
    "    # Iterate over each utterance in each dialogue\n",
    "    for dialogue in range(len(dialogue_acts_data)):\n",
    "        for utterance in range(len(dialogue_acts_data[dialogue])):\n",
    "\n",
    "            \n",
    "            dialogue_act = dialogue_acts_data[dialogue][utterance]\n",
    "            sentiment = sentiments[dialogue][utterance]\n",
    "\n",
    "            # If emotions list is empty, then emotions[dialogue][utterance][0] doens't exist,\n",
    "            # but if it has an emotion, then emotions[dialogue][utterance] is a list and not a string\n",
    "            if len(emotions[dialogue][utterance]) != 0:\n",
    "                emotion = emotions[dialogue][utterance][0]\n",
    "            else:\n",
    "                emotion = 'NaN'\n",
    "\n",
    "            # Adds emotion or increments emotion value in the correlations dict lists first dictionary\n",
    "            # If emotion is not found before for this dialogue act, it is added to the dict as a key, with a value of one.\n",
    "            # Otherwise the value for that key is incremented by one\n",
    "            # If emotion is nan, do nothing\n",
    "            if emotion == 'NaN':\n",
    "                a = 1\n",
    "            elif emotion in correlations[dialogue_act][0]:\n",
    "                correlations[dialogue_act][0][emotion] += 1\n",
    "            else:\n",
    "                correlations[dialogue_act][0][emotion] = 1\n",
    "\n",
    "            # Adds compound sentiment to the correlations dict lists second element\n",
    "            correlations[dialogue_act][1] += sentiment[\"compound\"]\n",
    "            correlations[dialogue_act][2] += 1\n",
    "\n",
    "    # This list stores lists, which have two elements. First is a dialogue act, second is the emotion that dialogue act has\n",
    "    # the highest correlation with, i.e. they appear the most together\n",
    "    highest_emotion_correlations = []\n",
    "\n",
    "    for dialogue_act in correlations:\n",
    "        \n",
    "        # Get average compound sentiment\n",
    "        correlations[dialogue_act][1] = correlations[dialogue_act][1] / correlations[dialogue_act][2]\n",
    "        print(\"Compound sentiment for \" + str(dialogue_act) + \" is: \" + str(correlations[dialogue_act][1]))\n",
    "\n",
    "        # For each dialogue act, find emotion that appears the most with it\n",
    "        if len(correlations[dialogue_act][0]) != 0:\n",
    "            highest_emotion_correlations.append([dialogue_act, max(correlations[dialogue_act][0], key=correlations[dialogue_act][0].get)])\n",
    "\n",
    "    return correlations, highest_emotion_correlations\n",
    "\n",
    "# The data in dialogue act dataframe is converted to be numerical in this format\n",
    "# none = 0\n",
    "#\"Emotion\" = 1\n",
    "#\"yAnswer\" = 2\n",
    "#\"Continuer\" = 3\n",
    "#\"whQuestion\" = 4\n",
    "#\"System\" = 5\n",
    "#\"Accept\" = 6\n",
    "#\"Clarify\" = 7\n",
    "#\"Emphasis\" = 8\n",
    "#\"nAnswer\" = 9\n",
    "#\"Greet\" = 10\n",
    "#\"Statement\" = 11\n",
    "#\"Reject\" = 12\n",
    "#\"Bye\" = 13\n",
    "#\"Other\" = 14\n",
    "#\"ynQuestion\" = 15\n",
    "def calculate_correlations(dialogue_acts_data_file, emotions_file, sentiments_file):\n",
    "\n",
    "    # Load data into dataframes\n",
    "    dialogue_acts_dataframe = pd.read_json(dialogue_acts_data_file)\n",
    "    emotions_dataframe = pd.read_json(emotions_file)\n",
    "    sentiments_dataframe= pd.read_json(sentiments_file)\n",
    "\n",
    "    # Change dialogue acts into numerical variables\n",
    "    # Let none be 0\n",
    "    dialogue_rows, dialogue_columns = dialogue_acts_dataframe.shape\n",
    "    for row in range(dialogue_rows):\n",
    "        for column in range(dialogue_columns):\n",
    "            if dialogue_acts_dataframe.iloc[row, column] == 'Emotion':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 1\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'yAnswer':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 2\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'Continuer':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 3\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'whQuestion':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 4\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'System':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 5\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'Accept':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 6\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'Clarify':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 7\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'Emphasis':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 8\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'nAnswer':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 9\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'Greet':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 10\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'Statement':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 11\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'Reject':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 12\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'Bye':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 13\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'Other':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 14\n",
    "            elif dialogue_acts_dataframe.iloc[row, column] == 'ynQuestion':\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 15\n",
    "            else:\n",
    "                dialogue_acts_dataframe.iloc[row, column] = 0\n",
    "    \n",
    "    # Let us only consider the compound sentiment of each utterance\n",
    "    sentiment_rows, sentiment_columns = sentiments_dataframe.shape\n",
    "    for row in range(sentiment_rows):\n",
    "        for column in range(sentiment_columns):\n",
    "            if isinstance(sentiments_dataframe.iloc[row, column], dict):\n",
    "                sentiments_dataframe.iloc[row, column] = sentiments_dataframe.iloc[row, column]['compound']\n",
    "            else:\n",
    "                sentiments_dataframe.iloc[row, column] = 0\n",
    "    \n",
    "    # Impute the mean to the missing values NOT DONE YET!!!\n",
    "    emotion_rows, emotion_columns = emotions_dataframe.shape\n",
    "    for row in range(emotion_rows):\n",
    "        for column in range(emotion_columns):\n",
    "            if emotions_dataframe.iloc[row, column] == None:\n",
    "                emotions_dataframe.iloc[row, column] = -1\n",
    "            elif len(emotions_dataframe.iloc[row, column]) == 0:\n",
    "                emotions_dataframe.iloc[row, column] = -1\n",
    "            else:\n",
    "                emotions_dataframe.iloc[row, column] = emotions_dataframe.iloc[row, column][0]\n",
    "\n",
    "    # Remove constant columns\n",
    "    # Find constant columns from the dialogue act dataframe\n",
    "    const_columns = []\n",
    "    for column in range(dialogue_columns):\n",
    "        if dialogue_acts_dataframe.iloc[:, column].std() == 0:\n",
    "            const_columns.append(column)\n",
    "\n",
    "    # Remove the constant columns from all dataframes\n",
    "    # Make two copies of the dialogue acts dataframe.\n",
    "    # From one, the sentiment constant columns will be removed\n",
    "    # and the emotion constant columns from the other\n",
    "    dialogue_acts_sentiment_dataframe = dialogue_acts_dataframe.drop(dialogue_acts_dataframe.columns[const_columns], axis=1)\n",
    "    dialogue_acts_emotion_dataframe = dialogue_acts_dataframe.drop(dialogue_acts_dataframe.columns[const_columns], axis=1)\n",
    "    sentiments_dataframe = sentiments_dataframe.drop(sentiments_dataframe.columns[const_columns], axis=1)\n",
    "    emotions_dataframe = emotions_dataframe.drop(emotions_dataframe.columns[const_columns], axis=1)\n",
    "\n",
    "    # Reset const_columns and set the rows and columns values to the new dimensions, after removing columns\n",
    "    const_columns = []\n",
    "    sentiment_rows, sentiment_columns = sentiments_dataframe.shape\n",
    "\n",
    "    # Find constant columns from the sentiments dataframe\n",
    "    for column in range(sentiment_columns):\n",
    "        if sentiments_dataframe.iloc[:, column].std() == 0:\n",
    "            const_columns.append(column)\n",
    "\n",
    "    # Remove the sentiment constant columns from sentiment & dialogue acts dataframes\n",
    "    dialogue_acts_sentiment_dataframe = dialogue_acts_sentiment_dataframe.drop(dialogue_acts_sentiment_dataframe.columns[const_columns], axis=1)\n",
    "    sentiments_dataframe = sentiments_dataframe.drop(sentiments_dataframe.columns[const_columns], axis=1)\n",
    "\n",
    "    # Reset const_columns and set the rows and columns values to the new dimensions, after removing columns\n",
    "    const_columns = []\n",
    "    emotion_rows, emotion_columns = emotions_dataframe.shape\n",
    "\n",
    "    # Find constant columns from the emotions dataframe\n",
    "    for column in range(emotion_columns):\n",
    "        if emotions_dataframe.iloc[:, column].std() == 0:\n",
    "            const_columns.append(column)\n",
    "\n",
    "    # Remove the emotion constant columns from emotion & dialogue acts dataframes\n",
    "    dialogue_acts_emotion_dataframe = dialogue_acts_emotion_dataframe.drop(dialogue_acts_emotion_dataframe.columns[const_columns], axis=1)\n",
    "    emotions_dataframe = emotions_dataframe.drop(emotions_dataframe.columns[const_columns], axis=1)\n",
    "\n",
    "    # Calculate the correlations\n",
    "    sentiment_correlation = dialogue_acts_sentiment_dataframe.corrwith(sentiments_dataframe)\n",
    "    emotion_correlation = dialogue_acts_emotion_dataframe.corrwith(emotions_dataframe)\n",
    "\n",
    "    return emotion_correlation, sentiment_correlation\n",
    "\n",
    "\n",
    "# Prints the matches\n",
    "correlations, highest_emotion_correlations = calculate_matching('all_dialogue_predictions.json', 'emos.json', 'sentiments.json')\n",
    "print(\"List of lists with the dialogue acts as the first element and the emotion most associated with that dialogue act as the second element\")\n",
    "print(highest_emotion_correlations)\n",
    "\n",
    "# Prints the correlations\n",
    "emotion_correlation, sentiment_correlation = calculate_correlations('all_dialogue_predictions.json', 'emos_upper_level.json', 'sentiments.json')\n",
    "print(\"Dialogue act / emotion correlations\")\n",
    "print(emotion_correlation)\n",
    "print(\"Dialogue act / sentiment correlations\")\n",
    "print(sentiment_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8 Machine learning model for emotion predictions\n",
    "\n",
    "If the kernel crashes when running this cell, use the task8.py file. When you have the dependencies, just run \"python task8.py\"\n",
    "\n",
    "Prints statistics and shows confusion matrices for all four machine learning models used in the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(utterance):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    vs = analyzer.polarity_scores(utterance)\n",
    "    if vs['compound'] <= -0.05:\n",
    "        return 0\n",
    "    elif vs['compound'] >= 0.05:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_pronouns(utterance):\n",
    "\n",
    "    tokens = word_tokenize(utterance)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    counter = 0\n",
    "    for pt in pos_tags:\n",
    "        if pt[1] == 'PRP' or pt[1] == 'PRP$':\n",
    "            counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "def get_negation(utterance):\n",
    "\n",
    "    negation_terms = ['no', 'not', 'never', 'none', 'nobody', \"don't\", \"can't\", 'neither']\n",
    "\n",
    "    tokens = word_tokenize(utterance)\n",
    "\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        if token in negation_terms:\n",
    "            counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "def get_dialogue_acts(dialogs):\n",
    "    with open('ijcnlp_dailydialog/dialogues_act.txt', 'r') as file:\n",
    "        acts = file.readlines()\n",
    "    \n",
    "    act_tags = {1: 'inform', 2: 'question', 3: 'directive', 4: 'commissive' }\n",
    "\n",
    "    a = []\n",
    "    for ac in acts:\n",
    "        temp = ac.split(' ')\n",
    "        temp = temp[:-1]\n",
    "        a.append(temp)\n",
    "    \n",
    "    d_acts = []\n",
    "    i = 0\n",
    "    for dialog in dialogs:\n",
    "        if len(dialog) != len(a[i]):\n",
    "            a[i].append(0)\n",
    "        for n in a[i]:\n",
    "            d_acts.append(int(n))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return d_acts\n",
    "\n",
    "def get_utterances(dialogs):\n",
    "\n",
    "    with open(\"ijcnlp_dailydialog/dialogues_emotion.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        emotion_numbers = file.readlines()\n",
    "\n",
    "    en = []\n",
    "    for e in emotion_numbers:\n",
    "        a = e.split(\" \")\n",
    "        a = a[:-1]\n",
    "        en.append(a)\n",
    "\n",
    "    utterances = []\n",
    "    emotions = []\n",
    "    i = 0\n",
    "    for dialog in dialogs:\n",
    "        if len(dialog) != len(en[i]):\n",
    "            en[i].append(0)\n",
    "        utterances.extend(dialog)\n",
    "        emotions.extend(en[i])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return utterances, emotions\n",
    "\n",
    "def save_features():\n",
    "    dialogs = get_dialogs()\n",
    "    utterances, emotions = get_utterances(dialogs)\n",
    "    acts = get_dialogue_acts(dialogs)\n",
    "\n",
    "    # Create feature vectors\n",
    "    features = []\n",
    "    i = 0\n",
    "    for utterance in utterances:\n",
    "        utterance_feature = []\n",
    "        utterance_feature.append(get_sentiment(utterance))\n",
    "        utterance_feature.append(get_pronouns(utterance))\n",
    "        utterance_feature.append(get_negation(utterance))\n",
    "        utterance_feature.append(acts[i])\n",
    "        features.append(utterance_feature)\n",
    "        i += 1\n",
    "    \n",
    "    with open('utterance_features.json', 'w') as file:\n",
    "        json.dump(features, file, indent=4)\n",
    "\n",
    "def load_dataset():\n",
    "    dialogs = get_dialogs()\n",
    "    utterances, emotions = get_utterances(dialogs)\n",
    "\n",
    "    with open('utterance_features.json', 'r') as file:\n",
    "        features = json.load(file)\n",
    "\n",
    "    for i in range(len(emotions)):\n",
    "        emotions[i] = int(emotions[i])\n",
    "\n",
    "    counter = 0\n",
    "    i = 0\n",
    "    while counter < 65000:\n",
    "        if emotions[i] == 0:\n",
    "            emotions.pop(i)\n",
    "            utterances.pop(i)\n",
    "            features.pop(i)\n",
    "            counter += 1\n",
    "            i -= 1\n",
    "        i += 1\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(utterances).toarray()\n",
    "    X = np.hstack([X, features])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, emotions, test_size=0.2)\n",
    "\n",
    "    return X_train, X_test, y_test, y_train\n",
    "\n",
    "def multinomialNB():\n",
    "    X_train, X_test, y_test, y_train = load_dataset()\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pr = clf.predict(X_test)\n",
    "\n",
    "    results(y_test, pr, 'Multinomial Naive Bayes')\n",
    "\n",
    "def randomForest():\n",
    "    X_train, X_test, y_test, y_train = load_dataset()\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pr = clf.predict(X_test)\n",
    "\n",
    "    results(y_test, pr, 'Random forest classifier')\n",
    "\n",
    "def ridgeClassifier():\n",
    "    X_train, X_test, y_test, y_train = load_dataset()\n",
    "    clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    pr = clf.predict(X_test)\n",
    "\n",
    "    results(y_test, pr, 'Ridge Classifier')\n",
    "\n",
    "def svm_cl():\n",
    "    X_train, X_test, y_test, y_train = load_dataset()\n",
    "    clf = svm.LinearSVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pr = clf.predict(X_test)\n",
    "\n",
    "    results(y_test, pr, 'SVM classifier')\n",
    "\n",
    "def results(y_test, pr, title):\n",
    "    accuracy = accuracy_score(y_test, pr)\n",
    "    precision = precision_score(y_test, pr, average='macro')\n",
    "    recall = recall_score(y_test, pr, average='macro', zero_division=np.nan)\n",
    "\n",
    "    unique, counts = np.unique(pr, return_counts=True)\n",
    "    for i in range(len(counts)):\n",
    "        print('Count ' + str(unique[i]) + ' : ' + str(counts[i]))\n",
    "    print(\"Accuracy: \" + str(round(accuracy ,3)))\n",
    "    print(\"Precision: \" + str(round(precision, 3)))\n",
    "    print(\"Recall: \" + str(round(recall, 3)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, pr, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_ticklabels([\"no emotion\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"])\n",
    "    ax.yaxis.set_ticklabels([\"no emotion\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "multinomialNB()\n",
    "randomForest()\n",
    "ridgeClassifier()\n",
    "svm_cl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
