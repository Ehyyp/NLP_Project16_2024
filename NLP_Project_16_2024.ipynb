{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description and information about the project goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\samik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\samik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\samik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import string\n",
    "import spacy\n",
    "from emotion import Emotion\n",
    "import json\n",
    "from wnaffect import WNAffect\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import heapq\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1\n",
    "Add task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ijcnlp_dailydialog/dialogues_topic.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m     save_dataframe_as_excel(topic_dialogue_data, filename)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Give topic number and name of file to save the data. The topic number can be given as a string or an integer.\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m \u001b[43mextract_and_save_topic_dialogue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mijcnlp_dailydialog/dialogues_text.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mijcnlp_dailydialog/dialogues_topic.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopic9data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 88\u001b[0m, in \u001b[0;36mextract_and_save_topic_dialogue\u001b[1;34m(path_to_dialogue_file, path_to_topic_file, topic_number, filename)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_and_save_topic_dialogue\u001b[39m(path_to_dialogue_file, path_to_topic_file, topic_number, filename):\n\u001b[1;32m---> 88\u001b[0m     topic_dialogue_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_topic_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_dialogue_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_topic_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     save_dataframe_as_excel(topic_dialogue_data, filename)\n",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m, in \u001b[0;36mcreate_topic_dataframe\u001b[1;34m(path_to_dialogue_file, path_to_topic_file, topic_number)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_topic_dataframe\u001b[39m(path_to_dialogue_file, path_to_topic_file, topic_number):\n\u001b[1;32m---> 44\u001b[0m     topic_dialogue \u001b[38;5;241m=\u001b[39m \u001b[43mextract_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_dialogue_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_topic_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m#split_dialogue = [line.split('__eou__') for line in topic_dialogue]\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     split_dialogue \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m, in \u001b[0;36mextract_topic\u001b[1;34m(path_to_dialogue_file, path_to_topic_file, topic_number)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_topic\u001b[39m(path_to_dialogue_file, path_to_topic_file, topic_number):\n\u001b[1;32m---> 24\u001b[0m     topic_lines \u001b[38;5;241m=\u001b[39m \u001b[43msave_topic_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_topic_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     topic_dialogue \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_to_dialogue_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36msave_topic_lines\u001b[1;34m(path_to_topic_file, topic_number)\u001b[0m\n\u001b[0;32m      5\u001b[0m topic_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m topic_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(topic_number)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_to_topic_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     10\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ijcnlp_dailydialog/dialogues_topic.txt'"
     ]
    }
   ],
   "source": [
    "# Extracts all line numbers of lines in the specified topic. The topic_number argument must be given as a string. For example: '9'\n",
    "# Topic number 9 is politics\n",
    "def save_topic_lines(path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_lines = []\n",
    "    topic_number = str(topic_number)\n",
    "\n",
    "    with open(path_to_topic_file, 'r') as file:\n",
    "        \n",
    "        i = 1\n",
    "\n",
    "        for line in file:\n",
    "            if line[0] == topic_number:\n",
    "                topic_lines.append(i)\n",
    "            \n",
    "            i = i + 1\n",
    "\n",
    "    return topic_lines\n",
    "\n",
    "# Extracts all dialogue lines from a specific topic\n",
    "# if topic is 'all', every topic is extracted\n",
    "def extract_topic(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_lines = save_topic_lines(path_to_topic_file, topic_number)\n",
    "    topic_dialogue = []\n",
    "\n",
    "    with open(path_to_dialogue_file, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file):\n",
    "\n",
    "            if topic_number == 'all':\n",
    "                topic_dialogue.append(line)\n",
    "\n",
    "            elif topic_number != 'all':\n",
    "                if line_number in topic_lines:\n",
    "                    topic_dialogue.append(line)\n",
    "\n",
    "    return topic_dialogue\n",
    "\n",
    "# Creates a pandas dataframe for the dialogue data in a specific topic\n",
    "# Rows are dialogue lines. They are in the same order as in the original dialogues_text.txt file\n",
    "# Columns are utterances in that dialogue.\n",
    "def create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_dialogue = extract_topic(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "\n",
    "\n",
    "    #split_dialogue = [line.split('__eou__') for line in topic_dialogue]\n",
    "\n",
    "    split_dialogue = []\n",
    "\n",
    "    for line in topic_dialogue:\n",
    "\n",
    "        split_line = line.split('__eou__')\n",
    "\n",
    "        for i in range(len(split_line)):\n",
    "            \n",
    "            if split_line[i][0] == \" \":\n",
    "\n",
    "                new_line = split_line[i][1:]\n",
    "                split_line[i] = new_line\n",
    "\n",
    "            if split_line[i][-1] == \" \":\n",
    "\n",
    "                new_line = split_line[i][:-1]\n",
    "                split_line[i] = new_line\n",
    "\n",
    "\n",
    "        split_dialogue.append(split_line)\n",
    "\n",
    "\n",
    "    topic_dialogue_data = pd.DataFrame(split_dialogue)\n",
    "\n",
    "    return topic_dialogue_data\n",
    "\n",
    "# Saves the dataframe in excel format\n",
    "# This is just for not having to write the annoying file format\n",
    "def save_dataframe_as_excel(data, filename):\n",
    "\n",
    "    if '.xlsx' not in filename:\n",
    "        filename = filename + '.xlsx'\n",
    "\n",
    "    data.to_excel(filename, header=False, index=False)\n",
    "\n",
    "# Does everything above. Extracts the topic, makes it into a dataframe and saves in excel format\n",
    "# if topic number is 'all', every topic is extracted\n",
    "def extract_and_save_topic_dialogue(path_to_dialogue_file, path_to_topic_file, topic_number, filename):\n",
    "\n",
    "    topic_dialogue_data = create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "    save_dataframe_as_excel(topic_dialogue_data, filename)\n",
    "\n",
    "# Give topic number and name of file to save the data. The topic number can be given as a string or an integer.\n",
    "extract_and_save_topic_dialogue('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', 9, 'topic9data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'topic9data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 166\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage number of negation words per utterance: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(avg_negation))\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Give name of the excel data file, where the dialogue data saved with previous block is stored\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m \u001b[43mprint_stats_from_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopic9data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 148\u001b[0m, in \u001b[0;36mprint_stats_from_excel\u001b[1;34m(name_of_excel_file)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name_of_excel_file:\n\u001b[0;32m    146\u001b[0m     name_of_excel_file \u001b[38;5;241m=\u001b[39m name_of_excel_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m dialogues \u001b[38;5;241m=\u001b[39m \u001b[43mopen_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_of_excel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m vocab \u001b[38;5;241m=\u001b[39m vocabulary_size(dialogues)\n\u001b[0;32m    150\u001b[0m utterances \u001b[38;5;241m=\u001b[39m count_utterances(dialogues)\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mopen_process_data\u001b[1;34m(name_of_excel_file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_process_data\u001b[39m(name_of_excel_file):\n\u001b[1;32m----> 4\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_of_excel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     rows, columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      6\u001b[0m     dialogues \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'topic9data.xlsx'"
     ]
    }
   ],
   "source": [
    "# Open the data that was made with task1_save_topic.py\n",
    "def open_process_data(name_of_excel_file):\n",
    "    \n",
    "    data = pd.read_excel(name_of_excel_file)\n",
    "    rows, columns = data.shape\n",
    "    dialogues = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "\n",
    "        utterances = []\n",
    "\n",
    "        for column in range(columns):\n",
    "            if type(data.iat[row, column]) is str:\n",
    "                utterances.append(data.iat[row, column])\n",
    "            \n",
    "        dialogues.append(utterances)\n",
    "\n",
    "    return dialogues\n",
    "\n",
    "# Tokenizes, lowers and removes special characters from data\n",
    "def tokenize_data(dialogues):\n",
    "    \n",
    "    special = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '=', '+', '[', ']', '{', '}', ';', ':', '\"', \"'\", '<', '>', ',', '.', '/', '?', '\\\\', '|', '`', '~', '...']\n",
    "    tokenized_dialogues = []\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        tokenized_dialogue = []\n",
    "\n",
    "        for utterance in dialogue:\n",
    "            processed_tokenized_utterance = []\n",
    "            tokenized_utterance = word_tokenize(utterance)\n",
    "\n",
    "            for token in tokenized_utterance:\n",
    "                token.lower()\n",
    "                if (token not in special) and (len(token) != 1):\n",
    "                    processed_tokenized_utterance.append(token)\n",
    "                \n",
    "            tokenized_dialogue.append(processed_tokenized_utterance)\n",
    "        \n",
    "        tokenized_dialogues.append(tokenized_dialogue)\n",
    "    \n",
    "    return tokenized_dialogues\n",
    "\n",
    "# Calculates the vocabulary size for data. Data is given in the form that tokenize_data returns it\n",
    "def vocabulary_size(dialogues):\n",
    "    \n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    counted_words = []\n",
    "    vocabulary_size = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            for token in utterance:\n",
    "                if token not in counted_words:\n",
    "                    vocabulary_size += 1\n",
    "                    counted_words.append(token)\n",
    "                    #print(\"Unique token: \" + token)\n",
    "\n",
    "    return vocabulary_size\n",
    "\n",
    "# Calculates the number of utterances for a dialogue\n",
    "def count_utterances(dialogues):\n",
    "\n",
    "    num_of_utterances = 0\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        for utterance in dialogue:\n",
    "            num_of_utterances += 1\n",
    "\n",
    "    return num_of_utterances\n",
    "\n",
    "# Count average tokens per utterance for dialogue\n",
    "def count_avg_tokens_per_utterance(dialogues):\n",
    "\n",
    "    num_of_utterances = count_utterances(dialogues)\n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    total_tokens = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            total_tokens += len(utterance)\n",
    "\n",
    "    avg_tokens_per_utterance = total_tokens / num_of_utterances\n",
    "\n",
    "    return avg_tokens_per_utterance\n",
    "\n",
    "\n",
    "# Uses NLTK part of speech tagger to identify pronouns, counts\n",
    "# the number of pronouns and then the average per utterance\n",
    "def avg_pronouns_per_utterance(dialogues):\n",
    "    \n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    pronoun_count = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            tagged_utterance = pos_tag(utterance)\n",
    "\n",
    "            for (token, prp_tag) in tagged_utterance:\n",
    "                if prp_tag == ('PRP' or 'PRP$'):\n",
    "                    pronoun_count += 1\n",
    "\n",
    "    num_of_utterances = count_utterances(dialogues)\n",
    "    avg_prp = pronoun_count / num_of_utterances\n",
    "\n",
    "    return avg_prp\n",
    "\n",
    "\n",
    "# Didn't find any clear resource for agreement or negation wording.\n",
    "# There is nltk.metrics.agreement, but it is not for counting agreement words\n",
    "# There is also the option to try and find negation/agreement related words through wordnet, but it would also find words that are not specifially negation/agreement words\n",
    "# The custom list of agreement/negation words is subject to change\n",
    "# choice = 1 counts average number of agreement words\n",
    "# choice = 2 does the same for negation words\n",
    "def avg_agreement_negation_per_utterance(dialogues, choice):\n",
    "\n",
    "    agreement_words = ['yes', 'ok', 'sure', 'okay', 'agreed', 'agree']\n",
    "    negation_words = ['no', 'not', \"don't\", \"can't\", 'neither', ]\n",
    "\n",
    "    if choice == 1:\n",
    "        words_to_count = agreement_words\n",
    "    elif choice == 2:\n",
    "        words_to_count = negation_words\n",
    "    else:\n",
    "        print(\"Second argument: 1 for agreement words, 2 for negation words\")\n",
    "        return 0\n",
    "\n",
    "    tokenized_dialogues = tokenize_data(dialogues)\n",
    "    num_of_utterances = count_utterances(dialogues)\n",
    "    num_words_to_count = 0\n",
    "\n",
    "    for dialogue in tokenized_dialogues:\n",
    "        for utterance in dialogue:\n",
    "            for token in utterance:\n",
    "                if token in words_to_count:\n",
    "                    num_words_to_count = num_words_to_count + 1\n",
    "    \n",
    "    avg_agreement_negation = num_words_to_count / num_of_utterances\n",
    "\n",
    "    return avg_agreement_negation\n",
    "\n",
    "# Prints all stats for a given topic\n",
    "def print_stats_from_excel(name_of_excel_file):\n",
    "\n",
    "    if '.xlsx' not in name_of_excel_file:\n",
    "        name_of_excel_file = name_of_excel_file + '.xlsx'\n",
    "\n",
    "    dialogues = open_process_data(name_of_excel_file)\n",
    "    vocab = vocabulary_size(dialogues)\n",
    "    utterances = count_utterances(dialogues)\n",
    "    tokens_per_utterance = count_avg_tokens_per_utterance(dialogues)\n",
    "    avg_prp = avg_pronouns_per_utterance(dialogues)\n",
    "    avg_agreement = avg_agreement_negation_per_utterance(dialogues, 1)\n",
    "    avg_negation = avg_agreement_negation_per_utterance(dialogues, 2)\n",
    "\n",
    "    print(\"Stats for file \\\"\" + name_of_excel_file + \"\\\":\")\n",
    "    print(\"Size of vocabulary: \" + str(vocab))\n",
    "    print(\"Number of utterances: \" + str(utterances))\n",
    "    print(\"Average number of tokens per utterance: \" + str(tokens_per_utterance))\n",
    "    print(\"Average number of pronouns per utterance: \" + str(avg_prp))\n",
    "    print(\"Average number of agreement words per utterance: \" + str(avg_agreement))\n",
    "    print(\"Average number of negation words per utterance: \" + str(avg_negation))\n",
    "\n",
    "\n",
    "# Give name of the excel data file, where the dialogue data saved with previous block is stored\n",
    "print_stats_from_excel('topic9data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2\n",
    "Add task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_ent_tag_per_utterance\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Give name of the excel data file, where the dialogue data is saved\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m ent_tags_avg \u001b[38;5;241m=\u001b[39m \u001b[43mavg_person_organization_entity_tags_per_utterance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopic9data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage number of person/organization named-entities per utterance: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(ent_tags_avg))\n",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m, in \u001b[0;36mavg_person_organization_entity_tags_per_utterance\u001b[1;34m(excel_file_name)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m excel_file_name:\n\u001b[0;32m      6\u001b[0m     excel_file_name \u001b[38;5;241m=\u001b[39m excel_file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m entity_tagger \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m dialogues \u001b[38;5;241m=\u001b[39m open_process_data(excel_file_name)\n\u001b[0;32m     10\u001b[0m num_entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# Opens an excel data file saved in the format that task1_save_topic.py saves and\n",
    "# calculates the entity tags per utterance\n",
    "def avg_person_organization_entity_tags_per_utterance(excel_file_name):\n",
    "\n",
    "    if '.xlsx' not in excel_file_name:\n",
    "        excel_file_name = excel_file_name + '.xlsx'\n",
    "\n",
    "    entity_tagger = spacy.load(\"en_core_web_md\")\n",
    "    dialogues = open_process_data(excel_file_name)\n",
    "    num_entities = 0\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        for utterance in dialogue:\n",
    "            entity_tagged_utterance = entity_tagger(utterance)\n",
    "\n",
    "            for entity in entity_tagged_utterance.ents:\n",
    "                if entity.label_ == (\"ORG\" or \"PERSON\"):\n",
    "                    num_entities += 1\n",
    "\n",
    "    num_utterances = count_utterances(dialogues)\n",
    "    avg_ent_tag_per_utterance = num_entities / num_utterances\n",
    "\n",
    "    return avg_ent_tag_per_utterance\n",
    "\n",
    "# Give name of the excel data file, where the dialogue data is saved\n",
    "ent_tags_avg = avg_person_organization_entity_tags_per_utterance('topic9data')\n",
    "print(\"Average number of person/organization named-entities per utterance: \" + str(ent_tags_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3 Emotion analysis using WNAffect\n",
    "\n",
    "Gives result for approaches A1 and A2 discussed in the report.\n",
    "\n",
    "In order to running this task, wordnet-1.6 and wn-domains-3.2 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WNAffect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 111\u001b[0m\n\u001b[0;32m    107\u001b[0m             emos\u001b[38;5;241m.\u001b[39mappend(u)\n\u001b[0;32m    109\u001b[0m     validate_m(emos)\n\u001b[1;32m--> 111\u001b[0m wna \u001b[38;5;241m=\u001b[39m \u001b[43mWNAffect\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet-1.6/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwn-domains-3.2/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Results for approaches A1 and A2 (reference to report)\u001b[39;00m\n\u001b[0;32m    114\u001b[0m results_a1()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WNAffect' is not defined"
     ]
    }
   ],
   "source": [
    "# Loads and parses dialogues from a text file dialogues_text.txt\n",
    "# separating each utterance by __eou__ and returns a list of lists (one per dialogue).\n",
    "def get_dialogs():\n",
    "\n",
    "    with open(\"dialogues_text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        dialogs = file.readlines()\n",
    "\n",
    "    parsed_dialogs = []\n",
    "    for dialog in dialogs:\n",
    "        d = dialog.split(\"__eou__\")\n",
    "        d = d[:-1]\n",
    "        parsed_dialogs.append(d)\n",
    "\n",
    "    return parsed_dialogs\n",
    "\n",
    "# Tokenizes each utterance and tags each word’s part of speech (POS).\n",
    "# Queries WNAffect to get emotions for each word in the utterance based on POS tags, accumulating any detected emotions in a list.\n",
    "# Returns list of emotions for utterance\n",
    "def get_emotions(utterance):\n",
    "\n",
    "    tokens = word_tokenize(utterance)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    emotions = []\n",
    "    for i in range(len(tokens)):\n",
    "        emo = wna.get_emotion(tokens[i], pos_tags[i][1])\n",
    "        if emo != None:\n",
    "            emotions.append(emo.name)\n",
    "            Emotion.printTree(Emotion.emotions[emo.name])\n",
    "            parent = emo.get_level(emo.level - 1)\n",
    "            print(\"parent: \" + parent.name)\n",
    "\n",
    "    return emotions\n",
    "\n",
    "# Calculates Accuracy, precision and recall scores for predicted WNAffect emotions.\n",
    "def validate_m(emotions):\n",
    "    \n",
    "    emotion_tags = get_emotions(get_dialogs())\n",
    "\n",
    "    emo_tags = {0: \"no emotion\", 1: \"anger\", 2: \"disgust\", 3: \"fear\", 4: \"happiness\", 5: \"sadness\", 6: \"surprise\"}\n",
    "\n",
    "    y_true = []\n",
    "    for tag in emotion_tags:\n",
    "        y_true.append(emo_tags[int(tag)])\n",
    "\n",
    "    y_pred = get_pred_class(emotions, y_true)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=np.nan)\n",
    "\n",
    "    print(\"Accuracy: \" + str(round(accuracy ,3)))\n",
    "    print(\"Precision: \" + str(round(precision, 3)))\n",
    "    print(\"Recall: \" + str(round(recall, 3)))\n",
    "\n",
    "# Preprocessing and prediction of the class of the emotion from WNAffect\n",
    "def get_pred_class(emotions, y_true):\n",
    "\n",
    "    y_pred = []\n",
    "    for i in range(len(emotions)):\n",
    "        if y_true[i] in emotions[i]:\n",
    "            y_pred.append(y_true[i])\n",
    "        elif len(emotions[i]) == 0:\n",
    "            y_pred.append('no emotion')\n",
    "        elif 'negative-fear' in emotions[i] or 'ambiguous-fear' in emotions[i]:\n",
    "            if y_true[i] == 'fear':\n",
    "                y_pred.append('fear')\n",
    "            else:\n",
    "                y_pred.append(emotions[i][0])\n",
    "        else:\n",
    "            y_pred.append(emotions[i][0])\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Calls get_dialogs to load dialogues.\n",
    "# Saves emotions fro each utterance to emos.json\n",
    "def save_emotions():\n",
    "    dialogs = get_dialogs()\n",
    "\n",
    "    emotions = []\n",
    "    for dialog in dialogs:\n",
    "        dialog_emo = []\n",
    "        for utterance in dialog:\n",
    "            emo = get_emotions(utterance)\n",
    "            dialog_emo.append(emo)\n",
    "        emotions.append(dialog_emo)\n",
    "\n",
    "    with open(\"emos.json\", \"w\") as f:\n",
    "        json.dump(emotions, f, indent=4)\n",
    "\n",
    "def results_a1():\n",
    "    with open(\"emos.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    emos = []\n",
    "    for d in data:\n",
    "        for u in d:\n",
    "            emos.append(u)\n",
    "\n",
    "    validate_m(emos)\n",
    "\n",
    "def results_a2():\n",
    "    with open(\"emos_upperlevel.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    emos = []\n",
    "    for d in data:\n",
    "        for u in d:\n",
    "            emos.append(u)\n",
    "\n",
    "    validate_m(emos)\n",
    "\n",
    "wna = WNAffect('wordnet-1.6/', 'wn-domains-3.2/')\n",
    "\n",
    "# Results for approaches A1 and A2 (reference to report)\n",
    "results_a1()\n",
    "results_a2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4 Sentiment analysis with Vader sentiment analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dialogues_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Comment here what this does\u001b[39;00m\n\u001b[1;32m      3\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m----> 4\u001b[0m dialogs \u001b[38;5;241m=\u001b[39m \u001b[43mget_dialogs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(analyzer.polarity_scores(\"Isn’t he the best instructor? I think he’s so hot. Wow! I really feel energized, dont’t you?\"))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dialog_sentiments \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mget_dialogs\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dialogs\u001b[39m():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdialogues_text.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         dialogs \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      8\u001b[0m     parsed_dialogs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/NLP/NLP_Project16_2024/project16env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dialogues_text.txt'"
     ]
    }
   ],
   "source": [
    "# Saves Vader sentiment analyser results to sentiments.json\n",
    "def save_sentiment():\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    dialogs = get_dialogs()\n",
    "\n",
    "    dialog_sentiments = []\n",
    "    for dialog in dialogs:\n",
    "        utterances = []\n",
    "        for utterance in dialog:\n",
    "            vs = analyzer.polarity_scores(utterance)\n",
    "            utterances.append(vs)\n",
    "            print(\"{:-<65} {}\".format(utterance, str(vs)))\n",
    "        dialog_sentiments.append(utterances)\n",
    "\n",
    "    with open(\"sentiments.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(dialog_sentiments, file, indent=4)\n",
    "\n",
    "# Reads sentiments.json and shows compound distribution of the results\n",
    "def results():\n",
    "\n",
    "    with open(\"sentiments.json\", 'r') as file:\n",
    "        sentiments = json.load(file)\n",
    "    \n",
    "    compounds = []\n",
    "    for dialog in sentiments:\n",
    "        for sentiment in dialog:\n",
    "            compounds.append(sentiment['compound'])\n",
    "\n",
    "    bins = np.linspace(-1,1)\n",
    "    data = compounds\n",
    "    plt.hist(data, bins=bins, edgecolor='black')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Vader sentiment analysis')\n",
    "    plt.xlabel('Compound score')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5 Correlation study between the sentiment and emotion states\n",
    "\n",
    "Prints values and shows both with none value results and without none values.\n",
    "\n",
    "In order to running this task, wordnet-1.6 and wn-domains-3.2 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dialogues_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Comment here\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[43mcompare_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m, in \u001b[0;36mcompare_and_save\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         dialogs\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     62\u001b[0m     compared_index\u001b[38;5;241m.\u001b[39mappend(dialogs)\n\u001b[0;32m---> 64\u001b[0m save_to_excel(compared_index, upper_level, sentiments, \u001b[43mget_dialogs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mget_dialogs\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dialogs\u001b[39m():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdialogues_text.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         dialogs \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      8\u001b[0m     parsed_dialogs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/NLP/NLP_Project16_2024/project16env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dialogues_text.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "wna = WNAffect('wordnet-1.6/', 'wn-domains-3.2/')\n",
    "\n",
    "# Saves level 4 emotions (ambiguous, neutral, negative and positive) to the emos_upper_level.json file\n",
    "def save_upper_level_emotions():\n",
    "    dialogs = get_dialogs()\n",
    "\n",
    "    emotions = []\n",
    "    for dialog in dialogs:\n",
    "        dialog_emo = []\n",
    "        for utterance in dialog:\n",
    "            emo = get_emotions(utterance)\n",
    "            dialog_emo.append(emo)\n",
    "        emotions.append(dialog_emo)\n",
    "\n",
    "    with open(\"emos_upper_level.json\", \"w\") as f:\n",
    "        json.dump(emotions, f, indent=4)\n",
    "\n",
    "# Tokenizes each utterance and tags each word’s part of speech (POS).\n",
    "# Gets level 4 emotions for all emotions found with WNAffect\n",
    "def get_emotions(utterance):\n",
    "\n",
    "    tokens = word_tokenize(utterance)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    emotion_tags = []\n",
    "    for i in range(len(tokens)):\n",
    "        emo = wna.get_emotion(tokens[i], pos_tags[i][1])\n",
    "        if emo != None:\n",
    "            emotion = get_upper_level_emotion(emo)\n",
    "            emotion_tags.append(emotion)\n",
    "            # Emotion.printTree(Emotion.emotions[emo.name])\n",
    "            # parent = emo.get_level(emo.level - 1)\n",
    "            # print(\"parent: \" + parent.name)\n",
    "\n",
    "    return emotion_tags\n",
    "\n",
    "# Helper function for searching the WNAffect emotion tree to level 4\n",
    "def get_upper_level_emotion(emo):\n",
    "\n",
    "    parent = emo.get_level(emo.level - 1)\n",
    "    if emo.name == 'love':\n",
    "        for child in emo.children:\n",
    "            print(child.name)\n",
    "\n",
    "    while parent.name != \"negative-emotion\" and parent.name != \"positive-emotion\" and parent.name != \"positive-emotion\" and parent.name != \"ambiguous-emotion\" and parent.name != \"neutral-emotion\":\n",
    "        parent = emo.get_level(parent.level - 1)\n",
    "    \n",
    "    if parent.name == \"ambiguous-emotion\" or parent.name == \"neutral-emotion\":\n",
    "        return 0\n",
    "    elif parent.name == \"negative-emotion\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Compares level 4 emotions to Vader sentiments.\n",
    "def compare_and_save():\n",
    "\n",
    "    with open(\"emos_upper_level.json\", \"r\") as file:\n",
    "        upper_level = json.load(file)\n",
    "    \n",
    "    with open(\"sentiments.json\", \"r\") as file:\n",
    "        sentiments = json.load(file)\n",
    "\n",
    "    compared_index = []\n",
    "    for i in range(len(upper_level)):\n",
    "        dialogs = []\n",
    "        for j in range(len(upper_level[i])):\n",
    "            result = get_compared_result(sentiments[i][j][\"compound\"], upper_level[i][j])\n",
    "            dialogs.append(result)\n",
    "        compared_index.append(dialogs)\n",
    "\n",
    "    save_to_excel(compared_index, upper_level, sentiments, get_dialogs())\n",
    "\n",
    "# Saves compatibility index, emotion value, sentiment, and corresponding utterance to the excel file task5_data.xlsx \n",
    "def save_to_excel(compared_index, emotion_values, sentiments, dialogs):\n",
    "    data = {}\n",
    "    compability_index_list = []\n",
    "    emotion_values_list = []\n",
    "    sentiments_list = []\n",
    "    utterances_list = []\n",
    "    for i in range(len(compared_index)):\n",
    "        compability_index_list.extend(compared_index[i])\n",
    "        for j in range(len(compared_index[i])):\n",
    "            if len(emotion_values[i][j]) == 0:\n",
    "                emotion_values_list.append(\"None\")\n",
    "            else:\n",
    "                emotion_values_list.append(emotion_values[i][j])\n",
    "            sentiments_list.append(sentiments[i][j][\"compound\"])\n",
    "            utterances_list.append(dialogs[i][j])\n",
    "    data[\"compability index\"] = compability_index_list\n",
    "    data[\"emotion value\"] = emotion_values_list\n",
    "    data[\"sentiment\"] = sentiments_list\n",
    "    data[\"utterance\"] = utterances_list\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(\"task5_data.xlsx\", index=False)\n",
    "\n",
    "# Helper function for crating conpatibility index\n",
    "def get_compared_result(sentiment_value, emotion_values):\n",
    "    if sentiment_value >= 0.05:\n",
    "        if 1 in emotion_values:\n",
    "            if 0 not in emotion_values and -1 not in emotion_values:\n",
    "                return 1\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    elif sentiment_value <= -0.05:\n",
    "        if -1 in emotion_values:\n",
    "            if 1 not in emotion_values and 0 not in emotion_values:\n",
    "                return 1\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if 0 in emotion_values:\n",
    "            if 1 not in emotion_values and -1 not in emotion_values:\n",
    "                return 1\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Results with none values\n",
    "def results():\n",
    "    df = pd.read_excel('task5_data.xlsx')\n",
    "    correlation = df['compability index']\n",
    "\n",
    "    counts = correlation.value_counts().sort_index()\n",
    "\n",
    "    print('incompatible: ', counts[0])\n",
    "    print('partial compability: ', counts[0.5])\n",
    "    print('Full compability: ', counts[1])\n",
    "\n",
    "    bin_labels = ['Incompatibility', 'Partial compatibility', 'Full compatibility']\n",
    "\n",
    "    plt.bar(bin_labels, counts, edgecolor='black')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Correlation distribution')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Results with no none values\n",
    "def results_none_removed():\n",
    "    df = pd.read_excel('task5_data.xlsx')\n",
    "    correlation = df['compability index']\n",
    "    emo_value = df['emotion value']\n",
    "\n",
    "    no_none_correlation = []\n",
    "    for i in range(len(correlation)):\n",
    "        if not pd.isna(emo_value[i]):\n",
    "            no_none_correlation.append(correlation[i])\n",
    "    \n",
    "    nncor = {}\n",
    "    nncor['compability index'] = no_none_correlation\n",
    "    df = pd.DataFrame(nncor)\n",
    "\n",
    "    counts = df['compability index'].value_counts().sort_index()\n",
    "\n",
    "    print('incompatible: ', counts[0])\n",
    "    print('partial compability: ', counts[0.5])\n",
    "    print('Full compability: ', counts[1])\n",
    "\n",
    "    bin_labels = ['Incompatibility', 'Partial compatibility', 'Full compatibility']\n",
    "\n",
    "    plt.bar(bin_labels, counts, edgecolor='black')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Correlation distribution')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "results()\n",
    "results_none_removed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6\n",
    "Add task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to /home/protago/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy on the NLTK NPS corpus: 0.667\n"
     ]
    }
   ],
   "source": [
    "# Opens excel data and saves it into a multidimensional list\n",
    "# Use task1_save_topic.py to save topic dialogues in this excel form\n",
    "def open_process_data(name_of_data_file):\n",
    "    \n",
    "    data = pd.read_excel(name_of_data_file, header=None)\n",
    "    rows, columns = data.shape\n",
    "    dialogs = []\n",
    "\n",
    "    for row in range(rows):\n",
    "        utterances = []\n",
    "\n",
    "        for column in range(columns):\n",
    "            if (type(data.iat[row, column]) is str) & (data.iat[row, column] != \"\\n\"):\n",
    "                utterances.append(data.iat[row, column])\n",
    "            \n",
    "        dialogs.append(utterances)\n",
    "\n",
    "    return dialogs\n",
    "\n",
    "# Feature extraction function as specified in the NLTK organization book chapter 6, section 2.2\n",
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features\n",
    "\n",
    "# Trains the classifier as specified in the NLTK organization book chapter 6, section 2.2\n",
    "def train_NLTK_model():\n",
    "\n",
    "    nltk.download('nps_chat')\n",
    "    posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "\n",
    "    featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]\n",
    "    size = int(len(featuresets) * 0.1)\n",
    "    train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    print(\"Classifier accuracy on the NLTK NPS corpus: \" + str(nltk.classify.accuracy(classifier, test_set)))\n",
    "\n",
    "    return classifier\n",
    "\n",
    "# Classifies data from an excel file\n",
    "def classify_data(name_of_data_file):\n",
    "\n",
    "    if '.xlsx' not in name_of_data_file:\n",
    "        name_of_data_file = name_of_data_file + '.xlsx'\n",
    "\n",
    "    data = open_process_data(name_of_data_file)\n",
    "    classifier = train_NLTK_model()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for dialog in data:\n",
    "        dialog_predictions = []\n",
    "\n",
    "        for utterance in dialog:\n",
    "            utterance_features = dialogue_act_features(utterance)\n",
    "            prediction = classifier.classify(utterance_features)\n",
    "            dialog_predictions.append(prediction)\n",
    "\n",
    "        predictions.append(dialog_predictions)\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "# Saves predictions\n",
    "def save_to_json(predictions, saved_file_name):\n",
    "\n",
    "    with open(saved_file_name, \"w\") as file:\n",
    "        json.dump(predictions, file, indent=4)\n",
    "\n",
    "# Give name of the excel data file, where the dialogue data is saved\n",
    "predictions = classify_data('topic9data')\n",
    "save_to_json(predictions, 'topic9data_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7\n",
    "Add task description\n",
    "Pearson correlation and p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compound sentiment for Emotion is: -0.19733333333333333\n",
      "Compound sentiment for yAnswer is: 0.4125599510318254\n",
      "Compound sentiment for Continuer is: 0.14945422885572143\n",
      "Compound sentiment for whQuestion is: 0.10348610379266035\n",
      "Compound sentiment for System is: 0.06702743764172334\n",
      "Compound sentiment for Accept is: 0.36048351449275456\n",
      "Compound sentiment for Clarify is: 0.24550313324404716\n",
      "Compound sentiment for Emphasis is: 0.281300698757766\n",
      "Compound sentiment for nAnswer is: 0.0759587800369691\n",
      "Compound sentiment for Greet is: 0.019240740740740742\n",
      "Compound sentiment for Statement is: 0.19866943847343674\n",
      "Compound sentiment for Reject is: 0.12184455385363421\n",
      "Compound sentiment for Bye is: 0.17189492385786803\n",
      "Compound sentiment for Other is: 0.2135223988439305\n",
      "Compound sentiment for ynQuestion is: 0.1458641363683198\n",
      "[['yAnswer', 'stupefaction'], ['Continuer', 'stupefaction'], ['whQuestion', 'benevolence'], ['Accept', 'benevolence'], ['Clarify', 'stupefaction'], ['Emphasis', 'stupefaction'], ['nAnswer', 'stupefaction'], ['Greet', 'benevolence'], ['Statement', 'benevolence'], ['Reject', 'stupefaction'], ['Bye', 'benevolence'], ['Other', 'stupefaction'], ['ynQuestion', 'stupefaction']]\n"
     ]
    }
   ],
   "source": [
    "# Comment here\n",
    "def calculate_correlations(dialogue_acts_data_file, emotions_file, sentiments_file):\n",
    "\n",
    "    with open(dialogue_acts_data_file, 'r', encoding='utf-8') as file:\n",
    "        dialogue_acts_data = json.load(file)\n",
    "\n",
    "    with open(emotions_file, 'r', encoding='utf-8') as file:\n",
    "        emotions = json.load(file)\n",
    "\n",
    "    with open(sentiments_file, 'r', encoding='utf-8') as file:\n",
    "        sentiments = json.load(file)\n",
    "\n",
    "    # The first element in the list holds a dictionary that has emotions as keys and the number of times that\n",
    "    # emotion has appeared in the same utterance for that dialogue act as values\n",
    "    # The second element in the list holds the compound sentiment and third keeps track of how many compound\n",
    "    # sentiments were added together\n",
    "    correlations = {\n",
    "        \"Emotion\": [{}, 0, 0],\n",
    "        \"yAnswer\": [{}, 0, 0],\n",
    "        #\"yAnswer\" : [{}, 0, 0],\n",
    "        \"Continuer\": [{}, 0, 0],\n",
    "        \"whQuestion\": [{}, 0, 0],\n",
    "        \"System\": [{}, 0, 0],\n",
    "        \"Accept\": [{}, 0, 0],\n",
    "        \"Clarify\": [{}, 0, 0],\n",
    "        #\"Clarity\": [{}, 0, 0],\n",
    "        \"Emphasis\": [{}, 0, 0],\n",
    "        \"nAnswer\": [{}, 0, 0],\n",
    "        \"Greet\": [{}, 0, 0],\n",
    "        \"Statement\": [{}, 0, 0],\n",
    "        \"Reject\": [{}, 0, 0],\n",
    "        \"Bye\": [{}, 0, 0],\n",
    "        \"Other\" : [{}, 0, 0],\n",
    "        #\"Others\" : [{}, 0, 0],\n",
    "        \"ynQuestion\" : [{}, 0, 0],\n",
    "    }\n",
    "\n",
    "    # Iterate over each utterance in each dialogue\n",
    "    for dialogue in range(len(dialogue_acts_data)):\n",
    "        for utterance in range(len(dialogue_acts_data[dialogue])):\n",
    "\n",
    "            \n",
    "            dialogue_act = dialogue_acts_data[dialogue][utterance]\n",
    "            sentiment = sentiments[dialogue][utterance]\n",
    "\n",
    "            # If emotions list is empty, then emotions[dialogue][utterance][0] doens't exist,\n",
    "            # but if it has an emotion, then emotions[dialogue][utterance] is a list and not a string\n",
    "            if len(emotions[dialogue][utterance]) != 0:\n",
    "                emotion = emotions[dialogue][utterance][0]\n",
    "            else:\n",
    "                emotion = 'NaN'\n",
    "\n",
    "            # Adds emotion or increments emotion value in the correlations dict lists first dictionary\n",
    "            # If emotion is not found before for this dialogue act, it is added to the dict as a key, with a value of one.\n",
    "            # Otherwise the value for that key is incremented by one\n",
    "            # If emotion is nan, do nothing\n",
    "            if emotion == 'NaN':\n",
    "                a = 1\n",
    "            elif emotion in correlations[dialogue_act][0]:\n",
    "                correlations[dialogue_act][0][emotion] += 1\n",
    "            else:\n",
    "                correlations[dialogue_act][0][emotion] = 1\n",
    "\n",
    "            # Adds compound sentiment to the correlations dict lists second element\n",
    "            correlations[dialogue_act][1] += sentiment[\"compound\"]\n",
    "            correlations[dialogue_act][2] += 1\n",
    "\n",
    "    # This list stores lists, which have two elements. First is a dialogue act, second is the emotion that dialogue act has\n",
    "    # the highest correlation with, i.e. they appear the most together\n",
    "    highest_emotion_correlations = []\n",
    "\n",
    "    for dialogue_act in correlations:\n",
    "        \n",
    "        # Get average compound sentiment\n",
    "        correlations[dialogue_act][1] = correlations[dialogue_act][1] / correlations[dialogue_act][2]\n",
    "        print(\"Compound sentiment for \" + str(dialogue_act) + \" is: \" + str(correlations[dialogue_act][1]))\n",
    "\n",
    "        # For each dialogue act, find emotion that appears the most with it\n",
    "        if len(correlations[dialogue_act][0]) != 0:\n",
    "            highest_emotion_correlations.append([dialogue_act, max(correlations[dialogue_act][0], key=correlations[dialogue_act][0].get)])\n",
    "\n",
    "    return correlations, highest_emotion_correlations\n",
    "\n",
    "# Comment here\n",
    "correlations, highest_emotion_correlations = calculate_correlations('all_dialogues_predictions.json', 'emos.json', 'sentiments.json')\n",
    "print(highest_emotion_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8 Machine learning model for emotion predictions\n",
    "\n",
    "Prints statistics and shows confusion matrices for all four machine learning models used in the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 186\u001b[0m\n\u001b[0;32m    182\u001b[0m     ax\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mset_ticklabels([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manger\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisgust\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhappiness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msadness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurprise\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    183\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m--> 186\u001b[0m \u001b[43mmultinomialNB\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m randomForest()\n\u001b[0;32m    188\u001b[0m ridgeClassifier()\n",
      "Cell \u001b[1;32mIn[21], line 135\u001b[0m, in \u001b[0;36mmultinomialNB\u001b[1;34m()\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultinomialNB\u001b[39m():\n\u001b[1;32m--> 135\u001b[0m     X_train, X_test, y_test, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     clf \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m    137\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "Cell \u001b[1;32mIn[21], line 111\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m utterances, emotions \u001b[38;5;241m=\u001b[39m get_utterances(dialogs)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutterance_features.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 111\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(emotions)):\n\u001b[0;32m    114\u001b[0m     emotions[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(emotions[i])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "def get_sentiment(utterance):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    vs = analyzer.polarity_scores(utterance)\n",
    "    if vs['compound'] <= -0.05:\n",
    "        return 0\n",
    "    elif vs['compound'] >= 0.05:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_pronouns(utterance):\n",
    "\n",
    "    tokens = word_tokenize(utterance)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    counter = 0\n",
    "    for pt in pos_tags:\n",
    "        if pt[1] == 'PRP' or pt[1] == 'PRP$':\n",
    "            counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "def get_negation(utterance):\n",
    "\n",
    "    negation_terms = ['no', 'not', 'never', 'none', 'nobody', \"don't\", \"can't\", 'neither']\n",
    "\n",
    "    tokens = word_tokenize(utterance)\n",
    "\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        if token in negation_terms:\n",
    "            counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "def get_dialogue_acts(dialogs):\n",
    "    with open('dialogues_act.txt', 'r') as file:\n",
    "        acts = file.readlines()\n",
    "    \n",
    "    act_tags = {1: 'inform', 2: 'question', 3: 'directive', 4: 'commissive' }\n",
    "\n",
    "    a = []\n",
    "    for ac in acts:\n",
    "        temp = ac.split(' ')\n",
    "        temp = temp[:-1]\n",
    "        a.append(temp)\n",
    "    \n",
    "    d_acts = []\n",
    "    i = 0\n",
    "    for dialog in dialogs:\n",
    "        if len(dialog) != len(a[i]):\n",
    "            a[i].append(0)\n",
    "        for n in a[i]:\n",
    "            d_acts.append(int(n))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return d_acts\n",
    "\n",
    "def get_utterances(dialogs):\n",
    "\n",
    "    with open(\"dialogues_emotion.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        emotion_numbers = file.readlines()\n",
    "\n",
    "    en = []\n",
    "    for e in emotion_numbers:\n",
    "        a = e.split(\" \")\n",
    "        a = a[:-1]\n",
    "        en.append(a)\n",
    "\n",
    "    utterances = []\n",
    "    emotions = []\n",
    "    i = 0\n",
    "    for dialog in dialogs:\n",
    "        if len(dialog) != len(en[i]):\n",
    "            en[i].append(0)\n",
    "        utterances.extend(dialog)\n",
    "        emotions.extend(en[i])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return utterances, emotions\n",
    "\n",
    "def save_features():\n",
    "    dialogs = get_dialogs()\n",
    "    utterances, emotions = get_utterances(dialogs)\n",
    "    acts = get_dialogue_acts(dialogs)\n",
    "\n",
    "    # Create feature vectors\n",
    "    features = []\n",
    "    i = 0\n",
    "    for utterance in utterances:\n",
    "        utterance_feature = []\n",
    "        utterance_feature.append(get_sentiment(utterance))\n",
    "        utterance_feature.append(get_pronouns(utterance))\n",
    "        utterance_feature.append(get_negation(utterance))\n",
    "        utterance_feature.append(acts[i])\n",
    "        features.append(utterance_feature)\n",
    "        i += 1\n",
    "    \n",
    "    with open('utterance_features.json', 'w') as file:\n",
    "        json.dump(features, file, indent=4)\n",
    "\n",
    "def load_dataset():\n",
    "    dialogs = get_dialogs()\n",
    "    utterances, emotions = get_utterances(dialogs)\n",
    "\n",
    "    with open('utterance_features.json', 'r') as file:\n",
    "        features = json.load(file)\n",
    "\n",
    "    for i in range(len(emotions)):\n",
    "        emotions[i] = int(emotions[i])\n",
    "\n",
    "    counter = 0\n",
    "    i = 0\n",
    "    while counter < 65000:\n",
    "        if emotions[i] == 0:\n",
    "            emotions.pop(i)\n",
    "            utterances.pop(i)\n",
    "            features.pop(i)\n",
    "            counter += 1\n",
    "            i -= 1\n",
    "        i += 1\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(utterances).toarray()\n",
    "    X = np.hstack([X, features])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, emotions, test_size=0.2)\n",
    "\n",
    "    return X_train, X_test, y_test, y_train\n",
    "\n",
    "def multinomialNB():\n",
    "    X_train, X_test, y_test, y_train = load_dataset()\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pr = clf.predict(X_test)\n",
    "\n",
    "    results(y_test, pr, 'Multinomial Naive Bayes')\n",
    "\n",
    "def randomForest():\n",
    "    X_train, X_test, y_test, y_train = load_dataset()\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pr = clf.predict(X_test)\n",
    "\n",
    "    results(y_test, pr, 'Random forest classifier')\n",
    "\n",
    "def ridgeClassifier():\n",
    "    X_train, X_test, y_test, y_train = load_dataset()\n",
    "    clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    pr = clf.predict(X_test)\n",
    "\n",
    "    results(y_test, pr, 'Ridge Classifier')\n",
    "\n",
    "def svm_cl():\n",
    "    X_train, X_test, y_test, y_train = load_dataset()\n",
    "    clf = svm.LinearSVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pr = clf.predict(X_test)\n",
    "\n",
    "    results(y_test, pr, 'SVM classifier')\n",
    "\n",
    "def results(y_test, pr, title):\n",
    "    accuracy = accuracy_score(y_test, pr)\n",
    "    precision = precision_score(y_test, pr, average='macro')\n",
    "    recall = recall_score(y_test, pr, average='macro', zero_division=np.nan)\n",
    "\n",
    "    unique, counts = np.unique(pr, return_counts=True)\n",
    "    for i in range(len(counts)):\n",
    "        print('Count ' + str(unique[i]) + ' : ' + str(counts[i]))\n",
    "    print(\"Accuracy: \" + str(round(accuracy ,3)))\n",
    "    print(\"Precision: \" + str(round(precision, 3)))\n",
    "    print(\"Recall: \" + str(round(recall, 3)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, pr, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_ticklabels([\"no emotion\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"])\n",
    "    ax.yaxis.set_ticklabels([\"no emotion\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "multinomialNB()\n",
    "randomForest()\n",
    "ridgeClassifier()\n",
    "svm_cl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
