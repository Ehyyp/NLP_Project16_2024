{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description and information about the project go here\n",
    "\n",
    "I have at the moment done all in this jupyter notebook. We can also save these functions as regular python files and make the final product a command line version, but I think this is easiest for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is for saving the desired data into excel format\n",
    "\n",
    "\n",
    "# Extracts all line numbers of lines in the specified topic. The topic_number argument must be given as a string. For example: '9'\n",
    "# Topic number 9 is politics\n",
    "def save_topic_lines(path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_lines = []\n",
    "\n",
    "    with open(path_to_topic_file, 'r') as file:\n",
    "        \n",
    "        i = 1\n",
    "\n",
    "        for line in file:\n",
    "            if line[0] == topic_number:\n",
    "                topic_lines.append(i)\n",
    "            \n",
    "            i = i + 1\n",
    "\n",
    "    return topic_lines\n",
    "\n",
    "\n",
    "# Extracts all dialogue lines from a specific topic\n",
    "def extract_topic(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_lines = save_topic_lines(path_to_topic_file, topic_number)\n",
    "    topic_dialogue = []\n",
    "\n",
    "    with open(path_to_dialogue_file, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file):\n",
    "            if line_number in topic_lines:\n",
    "                topic_dialogue.append(line)\n",
    "\n",
    "    return topic_dialogue\n",
    "\n",
    "\n",
    "# Creates a pandas dataframe for the dialogue data in a specific topic\n",
    "# Rows are dialogue lines. They are in the same order as in the original dialogues_text.txt file\n",
    "# Columns are utterances in that dialogue.\n",
    "def create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_dialogue = extract_topic(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "    split_dialogue = [line.split('__eou__') for line in topic_dialogue]\n",
    "    topic_dialogue_data = pd.DataFrame(split_dialogue)\n",
    "\n",
    "    return topic_dialogue_data\n",
    "\n",
    "\n",
    "# Saves the dataframe in excel format\n",
    "# This is just for not having to write the annoying file format\n",
    "def save_dataframe_as_excel(data, filename):\n",
    "\n",
    "    if '.xlsx' not in filename:\n",
    "        filename = filename + '.xlsx'\n",
    "\n",
    "    data.to_excel(filename, header=False, index=False)\n",
    "\n",
    "\n",
    "# Does everything above. Extracts the topic, makes it into a dataframe and saves in excel format\n",
    "def extract_and_save_topic_dialogue(path_to_dialogue_file, path_to_topic_file, topic_number, filename):\n",
    "\n",
    "    topic_dialogue_data = create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "    save_dataframe_as_excel(topic_dialogue_data, filename)\n",
    "\n",
    "\n",
    "# For testing purposes\n",
    "\n",
    "#politics = save_topic_lines('ijcnlp_dailydialog/dialogues_topic.txt', '9')\n",
    "#print(politics)\n",
    "\n",
    "#politics_dialogues = extract_topic('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', '9')\n",
    "#print(politics_dialogues)\n",
    "\n",
    "#politics_dialogue_data = create_topic_dataframe('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', '9')\n",
    "#print(politics_dialogue_data)\n",
    "\n",
    "#save_dataframe_as_excel(politics_dialogue_data, 'testdata.xlsx')\n",
    "\n",
    "#extract_and_save_topic_dialogue('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', '9', 'politics_dialogue_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is for calculating stats for the topic data\n",
    "\n",
    "\n",
    "# Takes into dataframe and concatenates everything in it to be a single string. This is for tokenization and such\n",
    "def form_dialogue_string(dataframe):\n",
    "\n",
    "    dialogue_string = ''\n",
    "    rows, columns = dataframe.shape\n",
    "\n",
    "    for row in range(rows):\n",
    "        for column in range(columns):\n",
    "            if type(dataframe.iat[row, column]) is str:\n",
    "                dialogue_string = dialogue_string + dataframe.iat[row, column]\n",
    "\n",
    "    return dialogue_string\n",
    "\n",
    "\n",
    "# Remove punctuation, lowercase and tokenize\n",
    "# There still remains things like \"sure.it\" and \"t\", remove\n",
    "def preprocess_dialogue(dialogue):\n",
    "\n",
    "    stop = set(list(string.punctuation))\n",
    "\n",
    "    tokenized = word_tokenize(dialogue.lower())\n",
    "    processed_dialogue = [word for word in tokenized if word not in stop]\n",
    "\n",
    "    return processed_dialogue\n",
    "\n",
    "\n",
    "# Calculates the vocabulary size for a dataframe\n",
    "def vocabulary_size(dataframe):\n",
    "    \n",
    "    dialogue_string = form_dialogue_string(dataframe)\n",
    "    processed_dialogue = preprocess_dialogue(dialogue_string)\n",
    "    unique_tokens = set(processed_dialogue)\n",
    "    vocabulary_size = len(unique_tokens)\n",
    "\n",
    "    # For testing\n",
    "    #print(processed_dialogue)\n",
    "    #print(len(processed_dialogue))\n",
    "    #print(processed_dialogue)\n",
    "    #print(unique_tokens)\n",
    "\n",
    "    return vocabulary_size\n",
    "\n",
    "\n",
    "# Calculates the number of utterances for a dataframe\n",
    "def count_utterances(dataframe):\n",
    "\n",
    "    num_of_utterances = 0\n",
    "    rows, columns = dataframe.shape\n",
    "\n",
    "    for row in range(rows):\n",
    "        for column in range(columns):\n",
    "            if type(dataframe.iat[row, column]) is str:\n",
    "                num_of_utterances = num_of_utterances + 1\n",
    "\n",
    "    return num_of_utterances\n",
    "\n",
    "\n",
    "# Count average tokens per utterance from a dataframe\n",
    "def count_avg_tokens_per_utterance(dataframe):\n",
    "\n",
    "    num_of_utterances = count_utterances(dataframe)\n",
    "\n",
    "    dialogue_string = form_dialogue_string(dataframe)\n",
    "    processed_dialogue = preprocess_dialogue(dialogue_string)\n",
    "\n",
    "    avg_tokens_per_utterance = len(processed_dialogue) / num_of_utterances\n",
    "\n",
    "    return avg_tokens_per_utterance\n",
    "\n",
    "\n",
    "#\n",
    "# def avg_pronouns_per_utterance(dataframe):\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# def avg_agreement_per_utterance(dataframe):\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# def avg_negation_per_utterance(dataframe):\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# def create_stats_table(dataframe):\n",
    "\n",
    "\n",
    "\n",
    "# For testing purposes\n",
    "\n",
    "politics_dialogue_data = create_topic_dataframe('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', '9')\n",
    "\n",
    "#dialogue_string = form_dialogue_string(politics_dialogue_data)\n",
    "#print(dialogue_string)\n",
    "\n",
    "vocab = vocabulary_size(politics_dialogue_data)\n",
    "print(\"Size of vocabulary: \" + str(vocab))\n",
    "\n",
    "utterances = count_utterances(politics_dialogue_data)\n",
    "print(\"Number of utterances: \" + str(utterances))\n",
    "\n",
    "avg_tokens = count_avg_tokens_per_utterance(politics_dialogue_data)\n",
    "print(\"Average number of tokens per utterance: \" + str(avg_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
