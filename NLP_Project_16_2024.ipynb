{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description and information about the project go here\n",
    "\n",
    "I have at the moment done all in this jupyter notebook. We can also save these functions as regular python files and make the final product a command line version, but I think this is easiest for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hyyppa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hyyppa/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/hyyppa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/hyyppa/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is for saving the desired data into excel format\n",
    "\n",
    "\n",
    "# Extracts all line numbers of lines in the specified topic. The topic_number argument must be given as a string. For example: '9'\n",
    "# Topic number 9 is politics\n",
    "def save_topic_lines(path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_lines = []\n",
    "\n",
    "    with open(path_to_topic_file, 'r') as file:\n",
    "        \n",
    "        i = 1\n",
    "\n",
    "        for line in file:\n",
    "            if line[0] == topic_number:\n",
    "                topic_lines.append(i)\n",
    "            \n",
    "            i = i + 1\n",
    "\n",
    "    return topic_lines\n",
    "\n",
    "\n",
    "# Extracts all dialogue lines from a specific topic\n",
    "def extract_topic(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_lines = save_topic_lines(path_to_topic_file, topic_number)\n",
    "    topic_dialogue = []\n",
    "\n",
    "    with open(path_to_dialogue_file, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file):\n",
    "            if line_number in topic_lines:\n",
    "                topic_dialogue.append(line)\n",
    "\n",
    "    return topic_dialogue\n",
    "\n",
    "\n",
    "# Creates a pandas dataframe for the dialogue data in a specific topic\n",
    "# Rows are dialogue lines. They are in the same order as in the original dialogues_text.txt file\n",
    "# Columns are utterances in that dialogue.\n",
    "def create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_dialogue = extract_topic(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "    split_dialogue = [line.split('__eou__') for line in topic_dialogue]\n",
    "    topic_dialogue_data = pd.DataFrame(split_dialogue)\n",
    "\n",
    "    return topic_dialogue_data\n",
    "\n",
    "\n",
    "# Saves the dataframe in excel format\n",
    "# This is just for not having to write the annoying file format\n",
    "def save_dataframe_as_excel(data, filename):\n",
    "\n",
    "    if '.xlsx' not in filename:\n",
    "        filename = filename + '.xlsx'\n",
    "\n",
    "    data.to_excel(filename, header=False, index=False)\n",
    "\n",
    "\n",
    "# Does everything above. Extracts the topic, makes it into a dataframe and saves in excel format\n",
    "def extract_and_save_topic_dialogue(path_to_dialogue_file, path_to_topic_file, topic_number, filename):\n",
    "\n",
    "    topic_dialogue_data = create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "    save_dataframe_as_excel(topic_dialogue_data, filename)\n",
    "\n",
    "\n",
    "# For testing purposes\n",
    "\n",
    "#politics = save_topic_lines('ijcnlp_dailydialog/dialogues_topic.txt', '9')\n",
    "#print(politics)\n",
    "\n",
    "#politics_dialogues = extract_topic('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', '9')\n",
    "#print(politics_dialogues)\n",
    "\n",
    "#politics_dialogue_data = create_topic_dataframe('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', '9')\n",
    "#print(politics_dialogue_data)\n",
    "\n",
    "#save_dataframe_as_excel(politics_dialogue_data, 'testdata.xlsx')\n",
    "\n",
    "#extract_and_save_topic_dialogue('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', '9', 'politics_dialogue_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for topic number 9:\n",
      "Size of vocabulary: 1436\n",
      "Number of utterances: 1635\n",
      "Average number of tokens per utterance: 7.856269113149847\n",
      "Average number of pronouns per utterance: 0.746177370030581\n",
      "Average number of agreement words per utterance: 0.0782874617737003\n",
      "Average number of negation words per utterance: 0.0672782874617737\n"
     ]
    }
   ],
   "source": [
    "# This block is for calculating stats for the topic data\n",
    "\n",
    "\n",
    "# Takes into dataframe and concatenates everything in it to be a single string. This is for tokenization and such\n",
    "def form_dialogue_string(dataframe):\n",
    "\n",
    "    dialogue_string = ''\n",
    "    rows, columns = dataframe.shape\n",
    "\n",
    "    for row in range(rows):\n",
    "        for column in range(columns):\n",
    "            if type(dataframe.iat[row, column]) is str:\n",
    "                dialogue_string = dialogue_string + dataframe.iat[row, column]\n",
    "\n",
    "    return dialogue_string\n",
    "\n",
    "\n",
    "# Remove punctuation, lowercase and tokenize\n",
    "# There still remains things like \"sure.it\" and \"t\", remove\n",
    "def preprocess_dialogue(dialogue):\n",
    "\n",
    "    stop = set(list(string.punctuation))\n",
    "\n",
    "    tokenized = word_tokenize(dialogue.lower())\n",
    "    processed_dialogue = [word for word in tokenized if word not in stop]\n",
    "\n",
    "    return processed_dialogue\n",
    "\n",
    "\n",
    "# Calculates the vocabulary size for a dataframe\n",
    "def vocabulary_size(dataframe):\n",
    "    \n",
    "    dialogue_string = form_dialogue_string(dataframe)\n",
    "    processed_dialogue = preprocess_dialogue(dialogue_string)\n",
    "    unique_tokens = set(processed_dialogue)\n",
    "    vocabulary_size = len(unique_tokens)\n",
    "\n",
    "    # For testing\n",
    "    #print(processed_dialogue)\n",
    "    #print(len(processed_dialogue))\n",
    "    #print(processed_dialogue)\n",
    "    #print(unique_tokens)\n",
    "\n",
    "    return vocabulary_size\n",
    "\n",
    "\n",
    "# Calculates the number of utterances for a dataframe\n",
    "def count_utterances(dataframe):\n",
    "\n",
    "    num_of_utterances = 0\n",
    "    rows, columns = dataframe.shape\n",
    "\n",
    "    for row in range(rows):\n",
    "        for column in range(columns):\n",
    "            if type(dataframe.iat[row, column]) is str:\n",
    "                num_of_utterances = num_of_utterances + 1\n",
    "\n",
    "    return num_of_utterances\n",
    "\n",
    "\n",
    "# Count average tokens per utterance from a dataframe\n",
    "def count_avg_tokens_per_utterance(dataframe):\n",
    "\n",
    "    num_of_utterances = count_utterances(dataframe)\n",
    "\n",
    "    dialogue_string = form_dialogue_string(dataframe)\n",
    "    processed_dialogue = preprocess_dialogue(dialogue_string)\n",
    "\n",
    "    avg_tokens_per_utterance = len(processed_dialogue) / num_of_utterances\n",
    "\n",
    "    return avg_tokens_per_utterance\n",
    "\n",
    "\n",
    "# Uses NLTK part of speech tagger to identify pronouns, counts the number of pronouns and then the average per utterance\n",
    "def avg_pronouns_per_utterance(dataframe):\n",
    "\n",
    "    dialogue_string = form_dialogue_string(dataframe)\n",
    "    processed_dialogue = preprocess_dialogue(dialogue_string)\n",
    "\n",
    "    tagged_dialogue = pos_tag(processed_dialogue)\n",
    "    \n",
    "    pronoun_count = 0\n",
    "\n",
    "    for (token, prp_tag) in tagged_dialogue:\n",
    "        if prp_tag == ('PRP' or 'PRP$'):\n",
    "            pronoun_count = pronoun_count + 1\n",
    "\n",
    "    num_of_utterances = count_utterances(dataframe)\n",
    "    avg_prp = pronoun_count / num_of_utterances\n",
    "\n",
    "    return avg_prp\n",
    "\n",
    "\n",
    "# Didn't find any clear resource for agreement or negation wording.\n",
    "# There is nltk.metrics.agreement, but it is not for counting agreement words\n",
    "# There is also the option to try and find negation/agreement related words through wordnet, but it would also find words that are not specifially negation/agreement words\n",
    "# The custom list of agreement/negation words is subject to change\n",
    "# choice = 1 counts average number of agreement words\n",
    "# choice = 2 does the same for negation words\n",
    "def avg_agreement_negation_per_utterance(dataframe, choice):\n",
    "\n",
    "    agreement_words = ['yes', 'ok', 'sure', 'okay', 'agreed', 'agree']\n",
    "    negation_words = ['no', 'not', \"don't\", \"can't\", 'neither', ]\n",
    "\n",
    "    if choice == 1:\n",
    "        words_to_count = agreement_words\n",
    "    elif choice == 2:\n",
    "        words_to_count = negation_words\n",
    "    else:\n",
    "        print(\"Second argument: 1 for agreement words, 2 for negation words\")\n",
    "        return 0\n",
    "\n",
    "    dialogue_string = form_dialogue_string(dataframe)\n",
    "    processed_dialogue = preprocess_dialogue(dialogue_string)\n",
    "    num_of_utterances = count_utterances(dataframe)\n",
    "\n",
    "    num_words_to_count = 0\n",
    "\n",
    "    for word in processed_dialogue:\n",
    "        if word in words_to_count:\n",
    "            num_words_to_count = num_words_to_count + 1\n",
    "    \n",
    "    avg_agreement = num_words_to_count / num_of_utterances\n",
    "\n",
    "    return avg_agreement\n",
    "\n",
    "# Prints all stats for a given topic\n",
    "def create_stats_table(path_to_dialogue_file, path_to_topic_file, topic_number):\n",
    "\n",
    "    topic_data = create_topic_dataframe(path_to_dialogue_file, path_to_topic_file, topic_number)\n",
    "\n",
    "    vocab = vocabulary_size(topic_data)\n",
    "    utterances = count_utterances(topic_data)\n",
    "    tokens_per_utterance = count_avg_tokens_per_utterance(topic_data)\n",
    "    avg_prp = avg_pronouns_per_utterance(topic_data)\n",
    "    avg_agreement = avg_agreement_negation_per_utterance(topic_data, 1)\n",
    "    avg_negation = avg_agreement_negation_per_utterance(topic_data, 2)\n",
    "\n",
    "    print(\"Stats for topic number \" + topic_number + \":\")\n",
    "    print(\"Size of vocabulary: \" + str(vocab))\n",
    "    print(\"Number of utterances: \" + str(utterances))\n",
    "    print(\"Average number of tokens per utterance: \" + str(tokens_per_utterance))\n",
    "    print(\"Average number of pronouns per utterance: \" + str(avg_prp))\n",
    "    print(\"Average number of agreement words per utterance: \" + str(avg_agreement))\n",
    "    print(\"Average number of negation words per utterance: \" + str(avg_negation))\n",
    "\n",
    "    \n",
    "\n",
    "# For testing purposes\n",
    "\n",
    "#politics_dialogue_data = create_topic_dataframe('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', '9')\n",
    "\n",
    "#dialogue_string = form_dialogue_string(politics_dialogue_data)\n",
    "#print(dialogue_string)\n",
    "\n",
    "#vocab = vocabulary_size(politics_dialogue_data)\n",
    "#print(\"Size of vocabulary: \" + str(vocab))\n",
    "\n",
    "#utterances = count_utterances(politics_dialogue_data)\n",
    "#print(\"Number of utterances: \" + str(utterances))\n",
    "\n",
    "#avg_tokens = count_avg_tokens_per_utterance(politics_dialogue_data)\n",
    "#print(\"Average number of tokens per utterance: \" + str(avg_tokens))\n",
    "\n",
    "#politics_prp_avg = avg_pronouns_per_utterance(politics_dialogue_data)\n",
    "#print(\"Average number of pronouns per utterance: \" + str(politics_prp_avg))\n",
    "\n",
    "#politics_agreement_avg = avg_agreement_negation_per_utterance(politics_dialogue_data, 2)\n",
    "#print(\"Average number of agreement/negation words per utterance: \" + str(politics_agreement_avg))\n",
    "\n",
    "create_stats_table('ijcnlp_dailydialog/dialogues_text.txt', 'ijcnlp_dailydialog/dialogues_topic.txt', '9')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project16_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
